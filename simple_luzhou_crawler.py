#!/usr/bin/env python3
"""
ä¿¡ç¾©æˆ¿å±‹è˜†æ´²è¯å»ˆå¤§æ¨“ç°¡åŒ–ç‰ˆçˆ¬èŸ²
ä½¿ç”¨ requests å’ŒåŸºæœ¬å¥—ä»¶ï¼Œé¿å…è¤‡é›œä¾è³´
"""

import json
import re
import os
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin
import sys
from pathlib import Path

# å˜—è©¦åŒ¯å…¥å¥—ä»¶
try:
    import requests
    from bs4 import BeautifulSoup
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦å¥—ä»¶: {e}")
    print("è«‹å®‰è£: pip3 install requests beautifulsoup4 --user")
    sys.exit(1)

# åŠ å…¥å°ˆæ¡ˆè·¯å¾‘
sys.path.append(str(Path(__file__).parent))

try:
    from src.models.property import Property
    from src.utils.full_notion import create_full_notion_client
    from src.utils.logger import get_logger
except ImportError as e:
    print(f"âš ï¸  ç„¡æ³•è¼‰å…¥å°ˆæ¡ˆæ¨¡çµ„: {e}")
    print("å°‡ä½¿ç”¨ç°¡åŒ–æ¨¡å¼é‹è¡Œ...")
    Property = None


class SimpleSinyiCrawler:
    """ä¿¡ç¾©æˆ¿å±‹è¯å»ˆå¤§æ¨“ç°¡åŒ–ç‰ˆçˆ¬èŸ²ï¼ˆæ”¯æ´å¤šå€åŸŸï¼‰"""
    
    def __init__(self, district="luzhou"):
        self.base_url = "https://www.sinyi.com.tw"
        
        # å®šç¾©ä¸åŒå€åŸŸçš„æœå°‹é…ç½®
        self.districts = {
            "luzhou": {
                "name": "è˜†æ´²",
                "zip_code": "247",
                "search_url": "https://www.sinyi.com.tw/buy/list/3000-down-price/huaxia-dalou-type/20-up-balconyarea/3-5-roomtotal/NewTaipei-city/247-zip/default-desc",
                "notion_title": "ä¿¡ç¾©è˜†æ´²è¯å»ˆå¤§æ¨“ç‰©ä»¶æ¸…å–®"
            },
            "sanchong": {
                "name": "ä¸‰é‡",
                "zip_code": "241",
                "search_url": "https://www.sinyi.com.tw/buy/list/3000-down-price/huaxia-dalou-type/20-up-balconyarea/3-5-roomtotal/NewTaipei-city/241-zip/default-desc",
                "notion_title": "ä¿¡ç¾©ä¸‰é‡è¯å»ˆå¤§æ¨“ç‰©ä»¶æ¸…å–®"
            },
            "taipei": {
                "name": "å°åŒ—",
                "zip_code": "100-103-104-105-106-108-110-115",
                "search_url": "https://www.sinyi.com.tw/buy/list/3000-down-price/apartment-type/20-up-balconyarea/3-5-roomtotal/1-3-floor/Taipei-city/100-103-104-105-106-108-110-115-zip/default-desc",
                "notion_title": "ä¿¡ç¾©å°åŒ—å…¬å¯“ç‰©ä»¶æ¸…å–®"
            }
        }
        
        # è¨­å®šç•¶å‰å€åŸŸ
        if district not in self.districts:
            raise ValueError(f"ä¸æ”¯æ´çš„å€åŸŸ: {district}ã€‚æ”¯æ´çš„å€åŸŸ: {list(self.districts.keys())}")
        
        self.current_district = district
        self.district_config = self.districts[district]
        self.search_base_url = self.district_config["search_url"]
        
        print(f"ğŸ¯ è¨­å®šçˆ¬èŸ²å€åŸŸ: {self.district_config['name']}å€")
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        # è™•ç† SSL æ†‘è­‰å•é¡Œ
        self.session.verify = False
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs("data", exist_ok=True)
        os.makedirs("logs", exist_ok=True)
    
    def fetch_page(self, url: str, delay: float = 2.0) -> Optional[str]:
        """ç²å–é é¢å…§å®¹"""
        try:
            time.sleep(delay)  # ç¦®è²Œæ€§å»¶é²
            
            print(f"ğŸ” æ­£åœ¨ç²å–: {url}")
            response = self.session.get(url, timeout=30)
            
            if response.status_code == 200:
                print(f"âœ… æˆåŠŸç²å–é é¢ï¼Œå…§å®¹é•·åº¦: {len(response.text)}")
                return response.text
            else:
                print(f"âŒ HTTPéŒ¯èª¤ {response.status_code}: {url}")
                return None
                
        except Exception as e:
            print(f"âŒ ç²å–é é¢å¤±æ•— {url}: {str(e)}")
            return None
    
    def get_total_pages(self) -> int:
        """ç²å–ç¸½é æ•¸"""
        first_page_url = f"{self.search_base_url}/1"
        html = self.fetch_page(first_page_url)
        
        if not html:
            return 1
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # å°‹æ‰¾åˆ†é ç›¸é—œçš„æ–‡å­—
        page_text = soup.get_text()
        
        # å°‹æ‰¾é¡ä¼¼ "ç¬¬ 1 é ï¼Œå…± 5 é " çš„æ–‡å­—
        page_match = re.search(r'å…±\s*(\d+)\s*é ', page_text)
        if page_match:
            total_pages = int(page_match.group(1))
            print(f"ğŸ“„ æª¢æ¸¬åˆ°ç¸½é æ•¸: {total_pages}")
            return total_pages
        
        # å°‹æ‰¾ "ç¬¬Xé " çš„æ¨¡å¼
        page_match2 = re.search(r'ç¬¬\s*\d+\s*é ï¼Œå…±\s*(\d+)\s*é ', page_text)
        if page_match2:
            total_pages = int(page_match2.group(1))
            print(f"ğŸ“„ æª¢æ¸¬åˆ°ç¸½é æ•¸: {total_pages}")
            return total_pages
        
        # å°‹æ‰¾åˆ†é å°èˆªå…ƒç´ 
        pagination_selectors = [
            'ul.pagination a',
            '.pagination a',
            'nav a',
            '.page-numbers a',
            'a[href*="/buy/list/"]'
        ]
        
        max_page = 1
        for selector in pagination_selectors:
            try:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href', '')
                    # å°‹æ‰¾åŒ…å«é ç¢¼çš„URL
                    page_match = re.search(r'/(\d+)$', href)
                    if page_match:
                        page_num = int(page_match.group(1))
                        if 1 <= page_num <= 100:  # åˆç†çš„é æ•¸ç¯„åœ
                            max_page = max(max_page, page_num)
                            
                    # ä¹Ÿæª¢æŸ¥é€£çµæ–‡å­—æ˜¯å¦ç‚ºæ•¸å­—
                    link_text = link.get_text().strip()
                    if link_text.isdigit():
                        page_num = int(link_text)
                        if 1 <= page_num <= 100:
                            max_page = max(max_page, page_num)
            except:
                continue
        
        # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå˜—è©¦é€šéæª¢æŸ¥ä¸‹ä¸€é æ˜¯å¦å­˜åœ¨ä¾†ç¢ºå®šç¸½é æ•¸
        if max_page == 1:
            print("ğŸ“„ å˜—è©¦é€šéæª¢æŸ¥é é¢å­˜åœ¨æ€§ä¾†ç¢ºå®šç¸½é æ•¸...")
            for test_page in range(2, 21):  # æ¸¬è©¦åˆ°ç¬¬20é 
                test_url = f"{self.search_base_url}/{test_page}"
                print(f"   æª¢æŸ¥ç¬¬ {test_page} é ...")
                test_html = self.fetch_page(test_url, delay=1.0)
                
                if test_html:
                    test_soup = BeautifulSoup(test_html, 'html.parser')
                    # æª¢æŸ¥æ˜¯å¦æœ‰ç‰©ä»¶è³‡æ–™
                    property_links = test_soup.find_all('a', href=re.compile(r'/buy/house/'))
                    if property_links:
                        max_page = test_page
                        print(f"   âœ… ç¬¬ {test_page} é æœ‰è³‡æ–™")
                    else:
                        print(f"   âŒ ç¬¬ {test_page} é ç„¡è³‡æ–™ï¼Œåœæ­¢æª¢æŸ¥")
                        break
                else:
                    print(f"   âŒ ç¬¬ {test_page} é ç„¡æ³•å­˜å–ï¼Œåœæ­¢æª¢æŸ¥")
                    break
        
        print(f"ğŸ“„ ç¢ºå®šç¸½é æ•¸: {max_page}")
        return max_page
    
    def parse_property_list(self, html: str) -> List[Dict[str, Any]]:
        """è§£ææˆ¿å±‹åˆ—è¡¨é é¢"""
        properties = []
        soup = BeautifulSoup(html, 'html.parser')
        
        # å°‹æ‰¾åŒ…å« /buy/house/ çš„é€£çµ
        property_links = soup.find_all('a', href=re.compile(r'/buy/house/'))
        
        print(f"ğŸ  æ‰¾åˆ° {len(property_links)} å€‹ç‰©ä»¶é€£çµ")
        
        # å·²è™•ç†çš„ç‰©ä»¶IDï¼Œé¿å…é‡è¤‡
        processed_ids = set()
        
        for link in property_links:
            try:
                href = link.get('href', '')
                if not href:
                    continue
                
                # æå–ç‰©ä»¶ID
                url_match = re.search(r'/buy/house/([A-Za-z0-9]+)', href)
                if not url_match:
                    continue
                
                object_id = url_match.group(1)
                
                # é¿å…é‡è¤‡è™•ç†
                if object_id in processed_ids:
                    continue
                processed_ids.add(object_id)
                
                # æ‰¾åˆ°é€™å€‹é€£çµæ‰€åœ¨çš„å®¹å™¨
                container = link.find_parent(['div', 'article', 'section'])
                if not container:
                    container = link
                
                property_info = self.extract_property_info(container, object_id, href)
                if property_info:
                    properties.append(property_info)
                    print(f"âœ… è§£æç‰©ä»¶: {property_info.get('title', 'Unknown')[:30]}...")
                
            except Exception as e:
                print(f"âš ï¸  è§£æç‰©ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue
        
        return properties
    
    def extract_property_info(self, container, object_id: str, href: str) -> Optional[Dict[str, Any]]:
        """å¾å®¹å™¨ä¸­æå–æˆ¿å±‹è³‡è¨Š"""
        
        detail_url = urljoin(self.base_url, href)
        
        # å–å¾—å®¹å™¨å…§çš„æ‰€æœ‰æ–‡å­—
        container_text = container.get_text()
        
        # æå–æ¨™é¡Œ - é€šå¸¸æ˜¯é€£çµçš„æ–‡å­—
        title_elem = container.find('a', href=re.compile(r'/buy/house/'))
        raw_title = self.clean_text(title_elem.get_text()) if title_elem else "æœªçŸ¥ç‰©ä»¶"
        
        # å¦‚æœåŸå§‹æ¨™é¡Œå¤ªçŸ­ï¼Œå˜—è©¦å°‹æ‰¾æ›´è©³ç´°çš„æ¨™é¡Œ
        if len(raw_title) < 5:
            title_candidates = container.find_all(['h1', 'h2', 'h3', 'h4', 'h5'])
            for candidate in title_candidates:
                candidate_text = self.clean_text(candidate.get_text())
                if len(candidate_text) > len(raw_title):
                    raw_title = candidate_text
                    break
        
        # å¾å®Œæ•´æ¨™é¡Œä¸­æå–ç°¡æ½”çš„ç‰©ä»¶åç¨±
        title = self.extract_property_name(raw_title)
        
        # æå–åƒ¹æ ¼
        price = self.extract_price(container_text)
        
        # æå–åœ°å€
        address = self.extract_address(container_text)
        
        # æå–æˆ¿å‹è³‡è¨Š
        room_info = self.extract_room_info(container_text)
        
        # æå–åªæ•¸
        size_info = self.extract_size_info(container_text)
        
        # æå–æ¨“å±¤
        floor_info = self.extract_floor_info(container_text)
        
        # æå–å±‹é½¡
        age_info = self.extract_age_info(container_text)
        
        property_info = {
            'id': f"sinyi_{self.current_district}_{object_id}",
            'object_id': object_id,
            'title': title,
            'address': address,
            'district': f'{self.district_config["name"]}å€',
            'region': 'æ–°åŒ—å¸‚',
            'price': price,
            'total_price': price,
            'room_count': room_info.get('rooms', 3),
            'living_room_count': room_info.get('living_rooms', 2),
            'bathroom_count': room_info.get('bathrooms', 2),
            'size': size_info.get('total_size', 0),
            'main_area': size_info.get('main_area', 0),
            'floor': floor_info,
            'age': age_info,
            'building_type': 'è¯å»ˆ/å¤§æ¨“',
            'source_site': 'ä¿¡ç¾©æˆ¿å±‹',
            'source_url': detail_url,
            'property_type': 'sale',
            'features': [],
            'equipment': [],
            'image_urls': [],
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        return property_info
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡å­—"""
        if not text:
            return ""
        return re.sub(r'\s+', ' ', str(text)).strip()
    
    def extract_property_name(self, title: str) -> str:
        """å¾å®Œæ•´æ¨™é¡Œä¸­æå–ç°¡æ½”çš„ç‰©ä»¶åç¨±"""
        if not title:
            return "æœªçŸ¥ç‰©ä»¶"
        
        # ç§»é™¤å¸¸è¦‹çš„å‰ç¶´
        title = re.sub(r'^(åº—é•·æ¨è–¦|å°ˆä»»|ç¨å®¶|æ€¥å”®|å‡ºåƒ¹å°±è«‡|å¯çœ‹|æ–°æ¥|ç¨€æœ‰|æ¨è–¦)\s*[â˜…â¤ï¸â­âœ¿ãŠ£\[\]ï½œÂ·]*\s*', '', title)
        
        # å˜—è©¦æå–ç¤¾å€åç¨±
        patterns = [
            # æ¨¡å¼1: ç¤¾å€åç¨±é‡è¤‡å‡ºç¾çš„æƒ…æ³ (å¦‚: "æ¦®è€€å·´é»...æ¦®è€€å·´é»æ–°åŒ—å¸‚")
            r'([A-Za-z\u4e00-\u9fff]{3,15}).*?\1æ–°åŒ—å¸‚',
            # æ¨¡å¼2: æ˜ç¢ºçš„ç¤¾å€åç¨± (å¦‚: "å…¨çƒå˜‰å¹´è¯æ–°åŒ—å¸‚")
            r'([A-Za-z\u4e00-\u9fff]{3,15})æ–°åŒ—å¸‚',
            # æ¨¡å¼3: ç¤¾å€åç¨±åœ¨æœ€æœ«å°¾ (å¦‚: "æ£®æ´»å¤§å¸‚æ–°åŒ—å¸‚") 
            r'([A-Za-z\u4e00-\u9fff]{3,12})æ–°åŒ—å¸‚[^A-Za-z\u4e00-\u9fff]*$',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, title)
            if match:
                extracted = match.group(1).strip()
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ç¤¾å€åç¨±
                if 3 <= len(extracted) <= 15:
                    # é¿å…ä¸€äº›é€šç”¨è©å½™
                    avoid_words = ['ä¸‰æˆ¿', 'å››æˆ¿', 'é›»æ¢¯', 'è»Šä½', 'å¤§æ¨“', 'è¯å»ˆ', 'å»ºåª', 'å¹´è¯', 'å¹´å¤§']
                    if not any(word in extracted for word in avoid_words):
                        return extracted
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç¤¾å€åç¨±ï¼Œå˜—è©¦æå–æè¿°æ€§æ¨™é¡Œ
        descriptive_patterns = [
            # æè¿° + ç¤¾å€åç¨±
            r'^([^æ–°åŒ—å°åŒ—0-9]{5,20}?)(?:æ–°åŒ—|å°åŒ—)',
            # ç´”æè¿°æ€§æ¨™é¡Œ
            r'^([^æ–°åŒ—å°åŒ—0-9]{3,15}?)(?:[0-9]+å¹´|å»ºåª)',
        ]
        
        for pattern in descriptive_patterns:
            match = re.search(pattern, title)
            if match:
                extracted = match.group(1).strip()
                # æ¸…ç†çµå°¾çš„å¸¸è¦‹è©å½™
                extracted = re.sub(r'(è»Šä½|ä¸‰æˆ¿|å››æˆ¿|é›»æ¢¯|æ™¯è§€|åº­é™¢|ç¶ æ„|é‚Šé–“|é«˜æ¨“|æ–¹æ­£|ç¾å­¸|è±ªé‚¸)$', '', extracted)
                if 3 <= len(extracted) <= 20:
                    return extracted
        
        # æœ€å¾Œçš„å‚™ç”¨æ–¹æ¡ˆï¼šå–å‰15å€‹å­—ç¬¦
        short_title = title[:15]
        for separator in ['æ–°åŒ—', 'å°åŒ—', 'å»ºåª', 'å¹´', 'æˆ¿', 'è¬']:
            if separator in short_title:
                short_title = short_title.split(separator)[0]
                break
        
        return short_title.strip() if short_title.strip() else "æœªçŸ¥ç‰©ä»¶"
    
    def extract_price(self, text: str) -> float:
        """æå–åƒ¹æ ¼ï¼ˆè¬å…ƒï¼‰"""
        # å°‹æ‰¾åƒ¹æ ¼æ¨¡å¼
        price_patterns = [
            r'(\d{1,4}(?:,\d{3})*(?:\.\d+)?)\s*è¬',
            r'ç¸½åƒ¹[ï¼š:\s]*(\d{1,4}(?:,\d{3})*(?:\.\d+)?)',
            r'(\d{3,4})\s*è¬'
        ]
        
        for pattern in price_patterns:
            price_match = re.search(pattern, text)
            if price_match:
                price_str = price_match.group(1).replace(',', '')
                try:
                    return float(price_str)
                except:
                    continue
        
        return 0
    
    def extract_address(self, text: str) -> str:
        """æå–åœ°å€"""
        district_name = self.district_config["name"]
        
        # å°‹æ‰¾åŒ…å«è©²å€åŸŸçš„åœ°å€
        address_patterns = [
            f'(æ–°åŒ—å¸‚{district_name}å€[^ï¼Œ\\n]{{1,30}})',
            f'({district_name}å€[^ï¼Œ\\n]{{1,30}})',
            f'æ–°åŒ—å¸‚\\s*{district_name}å€\\s*([^ï¼Œ\\n]{{1,20}})'
        ]
        
        for pattern in address_patterns:
            match = re.search(pattern, text)
            if match:
                return self.clean_text(match.group(1))
        
        return f"{district_name}å€"
    
    def extract_room_info(self, text: str) -> Dict[str, int]:
        """æå–æˆ¿å‹è³‡è¨Š"""
        # å„ªå…ˆå°‹æ‰¾æ¨™æº–æ ¼å¼
        room_match = re.search(r'(\d+)æˆ¿(\d+)å»³(\d+)è¡›', text)
        if room_match:
            rooms = int(room_match.group(1))
            living_rooms = int(room_match.group(2))
            bathrooms = int(room_match.group(3))
            
            # åˆç†æ€§æª¢æŸ¥
            if 1 <= rooms <= 10 and 1 <= living_rooms <= 5 and 1 <= bathrooms <= 5:
                return {
                    'rooms': rooms,
                    'living_rooms': living_rooms,
                    'bathrooms': bathrooms
                }
        
        # å°‹æ‰¾å…¶ä»–æ ¼å¼
        room_match2 = re.search(r'(\d+)R(\d+)L(\d+)B', text)
        if room_match2:
            rooms = int(room_match2.group(1))
            living_rooms = int(room_match2.group(2))
            bathrooms = int(room_match2.group(3))
            
            if 1 <= rooms <= 10 and 1 <= living_rooms <= 5 and 1 <= bathrooms <= 5:
                return {
                    'rooms': rooms,
                    'living_rooms': living_rooms,
                    'bathrooms': bathrooms
                }
        
        # åªå°‹æ‰¾æˆ¿é–“æ•¸
        room_only_match = re.search(r'(\d+)\s*æˆ¿', text)
        if room_only_match:
            rooms = int(room_only_match.group(1))
            if 1 <= rooms <= 10:
                return {
                    'rooms': rooms,
                    'living_rooms': 2,  # é è¨­å€¼
                    'bathrooms': 2      # é è¨­å€¼
                }
        
        # é è¨­å€¼
        return {'rooms': 3, 'living_rooms': 2, 'bathrooms': 2}
    
    def extract_size_info(self, text: str) -> Dict[str, float]:
        """æå–åªæ•¸è³‡è¨Š"""
        # å»ºåª
        size_patterns = [
            r'å»ºåª[ï¼š:\s]*(\d+(?:\.\d+)?)',
            r'ç¸½åªæ•¸[ï¼š:\s]*(\d+(?:\.\d+)?)',
            r'(\d+(?:\.\d+)?)\s*åª'
        ]
        
        total_size = 0
        for pattern in size_patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    total_size = float(match.group(1))
                    break
                except:
                    continue
        
        # ä¸»å»ºç‰©
        main_match = re.search(r'ä¸»å»ºç‰©[ï¼š:\s]*(\d+(?:\.\d+)?)', text)
        main_area = float(main_match.group(1)) if main_match else total_size * 0.8
        
        return {
            'total_size': total_size,
            'main_area': main_area
        }
    
    def extract_floor_info(self, text: str) -> str:
        """æå–æ¨“å±¤è³‡è¨Š"""
        floor_patterns = [
            r'(\d+)æ¨“/(\d+)æ¨“',
            r'(\d+)F/(\d+)F',
            r'(\d+)æ¨“',
            r'(\d+)F'
        ]
        
        for pattern in floor_patterns:
            floor_match = re.search(pattern, text)
            if floor_match:
                if len(floor_match.groups()) >= 2 and floor_match.group(2):
                    return f"{floor_match.group(1)}æ¨“/{floor_match.group(2)}æ¨“"
                else:
                    return f"{floor_match.group(1)}æ¨“"
        
        return "æœªçŸ¥æ¨“å±¤"
    
    def extract_age_info(self, text: str) -> int:
        """æå–å±‹é½¡"""
        age_patterns = [
            r'å±‹é½¡[ï¼š:\s]*(\d+(?:\.\d+)?)\s*å¹´',
            r'(\d+(?:\.\d+)?)\s*å¹´å±‹',
            r'æ°‘åœ‹\s*(\d+)\s*å¹´'
        ]
        
        for pattern in age_patterns:
            age_match = re.search(pattern, text)
            if age_match:
                try:
                    age = float(age_match.group(1))
                    # å¦‚æœæ˜¯æ°‘åœ‹å¹´ä»½ï¼Œè½‰æ›ç‚ºå±‹é½¡
                    if pattern.startswith(r'æ°‘åœ‹') and age > 50:
                        current_year = datetime.now().year - 1911  # æ°‘åœ‹å¹´
                        age = current_year - age
                    return int(age)
                except:
                    continue
        
        return 0
    
    def crawl_all_pages(self, max_pages: int = None) -> List[Dict[str, Any]]:
        """çˆ¬å–æ‰€æœ‰é é¢çš„ç‰©ä»¶"""
        district_name = self.district_config["name"]
        print(f"ğŸ” é–‹å§‹çˆ¬å–ä¿¡ç¾©æˆ¿å±‹{district_name}è¯å»ˆå¤§æ¨“ç‰©ä»¶...")
        
        # ç²å–ç¸½é æ•¸
        detected_total_pages = self.get_total_pages()
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§é æ•¸é™åˆ¶ï¼Œå‰‡ä½¿ç”¨è¼ƒå°å€¼
        if max_pages is not None:
            total_pages = min(detected_total_pages, max_pages)
            print(f"ğŸ“„ æª¢æ¸¬åˆ° {detected_total_pages} é ï¼Œä½†é™åˆ¶ç‚º {max_pages} é ï¼Œå°‡çˆ¬å– {total_pages} é ")
        else:
            total_pages = detected_total_pages
            print(f"ğŸ“„ å°‡çˆ¬å–æ‰€æœ‰ {total_pages} é ")
        
        all_properties = []
        
        # çˆ¬å–æ¯ä¸€é 
        for page in range(1, total_pages + 1):
            page_url = f"{self.search_base_url}/{page}"
            print(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page}/{total_pages} é ...")
            
            html = self.fetch_page(page_url, delay=2.0)  # é©ç•¶å»¶é²é¿å…è¢«å°
            
            if html:
                page_properties = self.parse_property_list(html)
                
                # å¦‚æœç•¶å‰é é¢æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç‰©ä»¶ï¼Œå¯èƒ½æ˜¯åˆ°äº†æœ€å¾Œä¸€é 
                if not page_properties:
                    print(f"âš ï¸  ç¬¬ {page} é æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç‰©ä»¶ï¼Œå¯èƒ½å·²åˆ°é”æœ€å¾Œä¸€é ")
                    break
                
                all_properties.extend(page_properties)
                print(f"âœ… ç¬¬ {page} é æ‰¾åˆ° {len(page_properties)} å€‹ç‰©ä»¶")
                
                # æª¢æŸ¥æ˜¯å¦æ‰¾åˆ°é‡è¤‡çš„ç‰©ä»¶IDï¼ˆè¡¨ç¤ºå¯èƒ½å¾ªç’°åˆ°å·²çˆ¬éçš„é é¢ï¼‰
                if page > 1:
                    current_ids = {prop['object_id'] for prop in page_properties}
                    previous_ids = {prop['object_id'] for prop in all_properties[:-len(page_properties)]}
                    duplicate_ids = current_ids.intersection(previous_ids)
                    
                    if duplicate_ids:
                        print(f"âš ï¸  ç¬¬ {page} é ç™¼ç¾é‡è¤‡ç‰©ä»¶ï¼Œå¯èƒ½å·²åˆ°é”å¯¦éš›æœ€å¾Œä¸€é ")
                        # ç§»é™¤é‡è¤‡çš„ç‰©ä»¶
                        all_properties = all_properties[:-len(page_properties)]
                        break
                        
            else:
                print(f"âŒ ç¬¬ {page} é çˆ¬å–å¤±æ•—")
                if page > 1:  # å¦‚æœä¸æ˜¯ç¬¬ä¸€é å°±å¤±æ•—ï¼Œå¯èƒ½æ˜¯åˆ°äº†æœ€å¾Œ
                    print(f"âš ï¸  å¯èƒ½å·²åˆ°é”æœ€å¾Œä¸€é ")
                    break
        
        # å»é™¤é‡è¤‡ç‰©ä»¶ï¼ˆä»¥é˜²è¬ä¸€ï¼‰
        unique_properties = []
        seen_ids = set()
        for prop in all_properties:
            if prop['object_id'] not in seen_ids:
                unique_properties.append(prop)
                seen_ids.add(prop['object_id'])
        
        if len(unique_properties) != len(all_properties):
            print(f"âš ï¸  ç§»é™¤äº† {len(all_properties) - len(unique_properties)} å€‹é‡è¤‡ç‰©ä»¶")
        
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼ç¸½å…±æ‰¾åˆ° {len(unique_properties)} å€‹å”¯ä¸€ç‰©ä»¶")
        return unique_properties
    
    def save_to_local_file(self, properties: List[Dict[str, Any]], filename_prefix: str = None) -> str:
        """å„²å­˜åˆ°æœ¬åœ°æª”æ¡ˆ"""
        if filename_prefix is None:
            filename_prefix = f"{self.current_district}_houses"
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        json_filename = f"data/{filename_prefix}_{timestamp}.json"
        
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(properties, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“ å·²å„²å­˜åˆ°: {json_filename}")
        return json_filename
    
    def load_previous_data(self) -> List[Dict[str, Any]]:
        """è¼‰å…¥å‰ä¸€å¤©çš„è³‡æ–™ç”¨æ–¼æ¯”è¼ƒï¼ˆæ”¯æ´ GitHub Actionsï¼‰"""
        # å„ªå…ˆæª¢æŸ¥ GitHub Actions ä¸‹è¼‰çš„å‰ä¸€å¤©è³‡æ–™
        previous_data_dirs = ["./previous_data", "data"]
        
        print(f"ğŸ” æ­£åœ¨æœå°‹å‰ä¸€å¤©çš„è³‡æ–™...")
        print(f"  â€¢ ç›®æ¨™å€åŸŸ: {self.current_district}")
        print(f"  â€¢ æœå°‹ç›®éŒ„: {previous_data_dirs}")
        
        for data_dir in previous_data_dirs:
            if not os.path.exists(data_dir):
                print(f"  âŒ {data_dir} ç›®éŒ„ä¸å­˜åœ¨")
                continue
                
            print(f"  ğŸ” åœ¨ {data_dir} ç›®éŒ„ä¸­æœå°‹å‰ä¸€å¤©çš„è³‡æ–™...")
            
            # åˆ—å‡ºç›®éŒ„ä¸­çš„æ‰€æœ‰æª”æ¡ˆ
            try:
                files_in_dir = os.listdir(data_dir)
                print(f"     ğŸ“„ æ‰¾åˆ° {len(files_in_dir)} å€‹æª”æ¡ˆ: {files_in_dir}")
            except Exception as e:
                print(f"     âŒ ç„¡æ³•åˆ—å‡ºæª”æ¡ˆ: {e}")
                continue
            
            # å¦‚æœæ˜¯ previous_data ç›®éŒ„ï¼ˆGitHub Actions ä¸‹è¼‰çš„ï¼‰ï¼Œç›´æ¥å°‹æ‰¾å°æ‡‰å€åŸŸæª”æ¡ˆ
            if data_dir == "./previous_data":
                filename_prefix = f"{self.current_district}_houses"
                print(f"     ğŸ¯ æœå°‹æª”æ¡ˆå‰ç¶´: {filename_prefix}")
                
                for filename in files_in_dir:
                    if filename.startswith(filename_prefix) and filename.endswith('.json'):
                        filepath = os.path.join(data_dir, filename)
                        print(f"     âœ… æ‰¾åˆ°åŒ¹é…æª”æ¡ˆ: {filename}")
                        try:
                            with open(filepath, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                print(f"     ğŸ“‚ å¾ GitHub Actions artifacts è¼‰å…¥å‰ä¸€å¤©è³‡æ–™: {len(data)} å€‹ç‰©ä»¶")
                                return data
                        except Exception as e:
                            print(f"     âŒ è¼‰å…¥å‰ä¸€å¤©è³‡æ–™å¤±æ•—: {str(e)}")
            else:
                # åŸæœ¬çš„é‚è¼¯ï¼šå°‹æ‰¾æ˜¨å¤©æ—¥æœŸçš„æª”æ¡ˆ
                yesterday = datetime.now() - timedelta(days=1)
                yesterday_str = yesterday.strftime('%Y%m%d')
                filename_prefix = f"{self.current_district}_houses"
                target_pattern = f"{filename_prefix}_{yesterday_str}"
                
                print(f"     ğŸ¯ æœå°‹æ˜¨å¤©æ—¥æœŸæª”æ¡ˆæ¨¡å¼: {target_pattern}*.json")
                
                for filename in files_in_dir:
                    if filename.startswith(target_pattern) and filename.endswith('.json'):
                        filepath = os.path.join(data_dir, filename)
                        print(f"     âœ… æ‰¾åˆ°æ˜¨å¤©çš„æª”æ¡ˆ: {filename}")
                        try:
                            with open(filepath, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                print(f"     ğŸ“‚ è¼‰å…¥æ˜¨å¤©çš„è³‡æ–™: {len(data)} å€‹ç‰©ä»¶")
                                return data
                        except Exception as e:
                            print(f"     âŒ è¼‰å…¥æ˜¨å¤©è³‡æ–™å¤±æ•—: {str(e)}")
        
        print("ğŸ“‚ æœªæ‰¾åˆ°å‰ä¸€å¤©çš„è³‡æ–™")
        return []
    
    def compare_with_previous(self, current_properties: List[Dict[str, Any]], previous_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """èˆ‡å‰ä¸€å¤©çš„è³‡æ–™æ¯”è¼ƒï¼ˆå¢å¼·ç‰ˆï¼‰"""
        if not previous_data:
            return {
                'has_previous_data': False,
                'new_properties': current_properties,
                'removed_properties': [],
                'price_changed_properties': [],
                'unchanged_properties': [],
                'total_new': len(current_properties),
                'total_removed': 0,
                'total_price_changed': 0,
                'current_count': len(current_properties),
                'previous_count': 0,
                'message': 'é¦–æ¬¡çˆ¬å–ï¼Œæ‰€æœ‰ç‰©ä»¶éƒ½æ˜¯æ–°çš„'
            }
        
        # å»ºç«‹æ˜¨å¤©çš„ç‰©ä»¶æ˜ å°„ (ä½¿ç”¨åœ°å€+æˆ¿å‹+åªæ•¸ä½œç‚º key)
        previous_map = {}
        for prop in previous_data:
            key = self._generate_property_key(prop)
            previous_map[key] = prop
        
        # å»ºç«‹ä»Šå¤©çš„ç‰©ä»¶æ˜ å°„
        current_map = {}
        for prop in current_properties:
            key = self._generate_property_key(prop)
            current_map[key] = prop
        
        # æ‰¾å‡ºæ–°å¢çš„ç‰©ä»¶
        new_properties = []
        unchanged_properties = []
        price_changed_properties = []
        
        for key, current_prop in current_map.items():
            if key not in previous_map:
                # æ–°å¢çš„ç‰©ä»¶
                new_properties.append(current_prop)
            else:
                # å­˜åœ¨çš„ç‰©ä»¶ï¼Œæª¢æŸ¥åƒ¹æ ¼æ˜¯å¦è®Šå‹•
                previous_prop = previous_map[key]
                current_price = current_prop.get('price', 0)
                previous_price = previous_prop.get('price', 0)
                
                if abs(current_price - previous_price) > 0:  # åƒ¹æ ¼æœ‰è®Šå‹•
                    price_changed_properties.append({
                        'property': current_prop,
                        'old_price': previous_price,
                        'new_price': current_price,
                        'change': current_price - previous_price
                    })
                else:
                    # åƒ¹æ ¼ç„¡è®Šå‹•çš„ç‰©ä»¶
                    unchanged_properties.append(current_prop)
        
        # æ‰¾å‡ºä¸‹æ¶çš„ç‰©ä»¶
        removed_properties = []
        for key, previous_prop in previous_map.items():
            if key not in current_map:
                removed_properties.append(previous_prop)
        
        # è¨ˆç®—è®ŠåŒ–
        change = len(current_properties) - len(previous_data)
        
        return {
            'has_previous_data': True,
            'new_properties': new_properties,
            'removed_properties': removed_properties,
            'price_changed_properties': price_changed_properties,
            'unchanged_properties': unchanged_properties,
            'total_new': len(new_properties),
            'total_removed': len(removed_properties),
            'total_price_changed': len(price_changed_properties),
            'current_count': len(current_properties),
            'previous_count': len(previous_data),
            'change': change,
            'message': f'èˆ‡æ˜¨å¤©æ¯”è¼ƒï¼šæ–°å¢ {len(new_properties)} å€‹ã€ä¸‹æ¶ {len(removed_properties)} å€‹ã€è®Šåƒ¹ {len(price_changed_properties)} å€‹ç‰©ä»¶'
        }
    
    def _generate_property_key(self, prop: Dict[str, Any]) -> str:
        """ç”Ÿæˆç‰©ä»¶çš„å”¯ä¸€è­˜åˆ¥éµ"""
        address = prop.get('address', '').strip()
        room_count = prop.get('room_count', 0)
        size = prop.get('size', 0)
        main_area = prop.get('main_area', size)
        
        # ä½¿ç”¨åœ°å€ã€æˆ¿æ•¸ã€åªæ•¸ä½œç‚ºå”¯ä¸€è­˜åˆ¥
        return f"{address}_{room_count}_{main_area}"
    
    def upload_to_notion_simple(self, properties: List[Dict[str, Any]], comparison_data: Dict = None) -> bool:
        """å˜—è©¦ä¸Šå‚³åˆ° Notionï¼ˆåªä¸Šå‚³æ–°å¢å’Œè®Šå‹•çš„ç‰©ä»¶ï¼‰"""
        notion_token = os.getenv('NOTION_API_TOKEN', os.getenv('NOTION_TOKEN'))
        
        if not notion_token:
            print("âš ï¸  æœªè¨­å®š NOTION_API_TOKENï¼Œè·³é Notion ä¸Šå‚³")
            print("ğŸ’¡ è«‹è¨­å®šç’°å¢ƒè®Šæ•¸æˆ–åŸ·è¡Œ setup_notion.py")
            return False
        
        # èª¿è©¦ï¼šé¡¯ç¤ºæ¯”è¼ƒè³‡æ–™çš„è©³ç´°è³‡è¨Š
        print(f"\nğŸ” æ¯”è¼ƒè³‡æ–™èª¿è©¦è³‡è¨Š:")
        if comparison_data:
            print(f"  â€¢ has_previous_data: {comparison_data.get('has_previous_data', False)}")
            print(f"  â€¢ æ–°å¢ç‰©ä»¶æ•¸é‡: {len(comparison_data.get('new_properties', []))}")
            print(f"  â€¢ åƒ¹æ ¼è®Šå‹•ç‰©ä»¶æ•¸é‡: {len(comparison_data.get('price_changed_properties', []))}")
            print(f"  â€¢ ç¸½è¨ˆç›®å‰ç‰©ä»¶: {comparison_data.get('current_count', 0)}")
            print(f"  â€¢ ç¸½è¨ˆå‰ä¸€å¤©ç‰©ä»¶: {comparison_data.get('previous_count', 0)}")
        else:
            print(f"  â€¢ comparison_data ç‚º None")
        
        # æ±ºå®šè¦ä¸Šå‚³çš„ç‰©ä»¶
        if comparison_data and comparison_data.get('has_previous_data'):
            # å¦‚æœæœ‰å‰ä¸€å¤©è³‡æ–™ï¼Œåªä¸Šå‚³æ–°å¢å’Œåƒ¹æ ¼è®Šå‹•çš„ç‰©ä»¶
            properties_to_upload = []
            
            # æ–°å¢çš„ç‰©ä»¶
            if comparison_data.get('new_properties'):
                properties_to_upload.extend(comparison_data['new_properties'])
                print(f"ğŸ“ å°‡ä¸Šå‚³ {len(comparison_data['new_properties'])} å€‹æ–°å¢ç‰©ä»¶")
            
            # åƒ¹æ ¼è®Šå‹•çš„ç‰©ä»¶
            if comparison_data.get('price_changed_properties'):
                for change_info in comparison_data['price_changed_properties']:
                    prop = change_info['property'].copy()
                    # åœ¨æ¨™é¡Œä¸­æ¨™è¨»åƒ¹æ ¼è®Šå‹•
                    change_amount = change_info['change']
                    change_emoji = "ğŸ“ˆ" if change_amount > 0 else "ğŸ“‰"
                    prop['title'] = f"{prop['title']} {change_emoji} åƒ¹æ ¼è®Šå‹•: {change_info['old_price']:,}â†’{change_info['new_price']:,}è¬"
                    properties_to_upload.append(prop)
                print(f"ğŸ“ å°‡ä¸Šå‚³ {len(comparison_data['price_changed_properties'])} å€‹åƒ¹æ ¼è®Šå‹•ç‰©ä»¶")
            
            if not properties_to_upload:
                print("âœ… æ²’æœ‰æ–°å¢æˆ–è®Šå‹•çš„ç‰©ä»¶ï¼ŒNotion ç­†è¨˜ä¿æŒä¸è®Š")
                return True
                
            print(f"ğŸ“ ç¸½å…±ä¸Šå‚³ {len(properties_to_upload)} å€‹æœ‰è®ŠåŒ–çš„ç‰©ä»¶åˆ° Notion")
        else:
            # å¦‚æœæ²’æœ‰å‰ä¸€å¤©è³‡æ–™ï¼Œä¸Šå‚³æ‰€æœ‰ç‰©ä»¶
            properties_to_upload = properties
            print(f"ğŸ“ é¦–æ¬¡åŸ·è¡Œï¼Œå°‡ä¸Šå‚³æ‰€æœ‰ {len(properties)} å€‹ç‰©ä»¶åˆ° Notion")
        
        try:
            # å˜—è©¦åŒ¯å…¥ Notion åŠŸèƒ½
            from src.utils.full_notion import create_full_notion_client
            from src.models.property import Property
            
            print("ğŸ”„ æ­£åœ¨ä¸Šå‚³åˆ° Notion...")
            
            client = create_full_notion_client(notion_token)
            
            if not client.test_connection():
                print("âŒ Notion API é€£æ¥å¤±æ•—")
                return False
            
            # å°‡å­—å…¸è³‡æ–™è½‰æ›ç‚º Property å°è±¡
            property_objects = []
            for i, prop_dict in enumerate(properties_to_upload):
                try:
                    # ç”Ÿæˆå”¯ä¸€ ID
                    prop_id = f"{self.district_config['name']}_{i+1}_{prop_dict.get('title', '')[:10]}"
                    
                    prop = Property(
                        id=prop_id,
                        title=prop_dict.get('title', ''),
                        address=prop_dict.get('address', ''),
                        district=self.district_config['name'],
                        region='æ–°åŒ—å¸‚' if self.district_config['name'] != 'å°åŒ—' else 'å°åŒ—å¸‚',
                        price=int(prop_dict.get('price', 0)),
                        room_count=prop_dict.get('room_count', 3),
                        living_room_count=prop_dict.get('living_room_count', 2),
                        bathroom_count=prop_dict.get('bathroom_count', 2),
                        size=prop_dict.get('size', 0),
                        floor=str(prop_dict.get('floor', '')),
                        source_site="ä¿¡ç¾©æˆ¿å±‹",
                        source_url=prop_dict.get('source_url', ''),
                        property_type='sale'  # è²·å±‹
                    )
                    
                    # è¨­å®šè²·å±‹å°ˆç”¨æ¬„ä½
                    prop.total_price = int(prop_dict.get('price', 0))  # ç¸½åƒ¹
                    
                    # è¨­å®šå…¶ä»–å±¬æ€§ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                    if 'main_area' in prop_dict:
                        prop.main_area = prop_dict['main_area']
                    if 'unit_price' in prop_dict:
                        prop.unit_price = prop_dict['unit_price']
                    if 'total_floors' in prop_dict:
                        prop.total_floors = prop_dict['total_floors']
                    if 'age' in prop_dict:
                        prop.age = prop_dict['age']
                    if 'building_type' in prop_dict:
                        prop.building_type = prop_dict['building_type']
                    
                    property_objects.append(prop)
                except Exception as e:
                    print(f"âš ï¸  è½‰æ›ç‰©ä»¶å¤±æ•—: {e}")
                    continue
            
            # ä½¿ç”¨æ–°çš„å±¤ç´šçµæ§‹ä¸Šå‚³
            today = datetime.now()
            district_name = self._get_district_key()  # ç²å–è‹±æ–‡å€åŸŸåç¨±
            
            # ä½¿ç”¨æ–°çš„å€åŸŸç‰©ä»¶æ¸…å–®å‰µå»ºæ–¹æ³•ï¼Œå‚³å…¥æ¯”è¼ƒè³‡æ–™
            success = client.create_district_house_list(
                properties=property_objects,
                search_date=today,
                district_name=district_name,
                comparison=comparison_data  # å‚³å…¥æ¯”è¼ƒè³‡æ–™
            )
            
            if success:
                print("âœ… æˆåŠŸä¸Šå‚³åˆ° Notionï¼")
                return True
            else:
                print("âŒ Notion ä¸Šå‚³å¤±æ•—")
                return False
                
        except ImportError:
            print("âš ï¸  ç„¡æ³•è¼‰å…¥ Notion åŠŸèƒ½ï¼Œè·³éä¸Šå‚³")
            return False
        except Exception as e:
            print(f"âŒ Notion ä¸Šå‚³éŒ¯èª¤: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def _get_district_key(self) -> str:
        """ç²å–å€åŸŸçš„è‹±æ–‡éµå€¼"""
        district_map = {
            'è˜†æ´²': 'luzhou',
            'ä¸‰é‡': 'sanchong',
            'å°åŒ—': 'taipei'
        }
        return district_map.get(self.district_config['name'], 'unknown')

def main():
    """ä¸»ç¨‹å¼"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¿¡ç¾©æˆ¿å±‹è¯å»ˆå¤§æ¨“ç‰©ä»¶çˆ¬èŸ²')
    parser.add_argument('--district', 
                       choices=['luzhou', 'sanchong', 'taipei', 'all'], 
                       default='luzhou',
                       help='é¸æ“‡çˆ¬å–å€åŸŸ: luzhou(è˜†æ´²), sanchong(ä¸‰é‡), taipei(å°åŒ—), all(å…¨éƒ¨)')
    
    args = parser.parse_args()
    
    if args.district == 'all':
        # çˆ¬å–æ‰€æœ‰å€åŸŸ
        districts_to_crawl = ['luzhou', 'sanchong', 'taipei']
    else:
        districts_to_crawl = [args.district]
    
    print("ğŸ  ä¿¡ç¾©æˆ¿å±‹è¯å»ˆå¤§æ¨“ç‰©ä»¶çˆ¬èŸ²")
    print("=" * 50)
    
    for district in districts_to_crawl:
        print(f"\nğŸ¯ é–‹å§‹çˆ¬å– {district} å€åŸŸ...")
        print("-" * 30)
        
        try:
            crawler = SimpleSinyiCrawler(district=district)
            
            # 1. è¼‰å…¥å‰ä¸€å¤©çš„è³‡æ–™
            print("ğŸ“‚ è¼‰å…¥å‰ä¸€å¤©çš„è³‡æ–™...")
            previous_data = crawler.load_previous_data()
            
            # 2. çˆ¬å–ä»Šå¤©çš„è³‡æ–™ï¼ˆä¸é™åˆ¶é æ•¸ï¼Œçˆ¬å–æ‰€æœ‰é é¢ï¼‰
            properties = crawler.crawl_all_pages()  # ç§»é™¤ max_pages é™åˆ¶
            
            if not properties:
                print("âŒ æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•è³‡æ–™")
                continue
            
            # 3. èˆ‡å‰ä¸€å¤©æ¯”è¼ƒ
            comparison = crawler.compare_with_previous(properties, previous_data)
            print(f"ğŸ“ˆ {comparison['message']}")
            
            # ç‰¹åˆ¥é‡å°å°åŒ—å€åŸŸå¢åŠ è©³ç´°èª¿è©¦
            if district == 'taipei':
                print(f"\nğŸ” å°åŒ—å€åŸŸè©³ç´°æ¯”è¼ƒè³‡è¨Š:")
                print(f"  â€¢ has_previous_data: {comparison.get('has_previous_data', False)}")
                print(f"  â€¢ å‰ä¸€å¤©ç‰©ä»¶æ•¸: {comparison.get('previous_count', 0)}")
                print(f"  â€¢ ä»Šå¤©ç‰©ä»¶æ•¸: {comparison.get('current_count', 0)}")
                print(f"  â€¢ æ–°å¢ç‰©ä»¶æ•¸: {comparison.get('total_new', 0)}")
                print(f"  â€¢ ä¸‹æ¶ç‰©ä»¶æ•¸: {comparison.get('total_removed', 0)}")
                print(f"  â€¢ è®Šåƒ¹ç‰©ä»¶æ•¸: {comparison.get('total_price_changed', 0)}")
                
                if comparison.get('new_properties'):
                    print(f"  ğŸ†• æ–°å¢ç‰©ä»¶ç¯„ä¾‹:")
                    for i, prop in enumerate(comparison['new_properties'][:3], 1):
                        print(f"    {i}. {prop['title'][:30]}... - {prop['price']}è¬")
                
                if comparison.get('price_changed_properties'):
                    print(f"  ğŸ’° è®Šåƒ¹ç‰©ä»¶ç¯„ä¾‹:")
                    for i, change in enumerate(comparison['price_changed_properties'][:2], 1):
                        prop = change['property']
                        print(f"    {i}. {prop['title'][:30]}... : {change['old_price']} â†’ {change['new_price']}è¬")
                
                if not comparison.get('new_properties') and not comparison.get('price_changed_properties'):
                    print(f"  âœ… å°åŒ—å€åŸŸä»Šå¤©æ²’æœ‰æ–°å¢æˆ–è®Šåƒ¹ç‰©ä»¶")
            
            if comparison['new_properties']:
                print("\nğŸ†• æ–°å¢çš„ç‰©ä»¶:")
                for i, prop in enumerate(comparison['new_properties'][:5], 1):  # åªé¡¯ç¤ºå‰5å€‹
                    print(f"  {i}. {prop['title'][:30]}... - {prop['price']}è¬å…ƒ")
                    print(f"     ğŸ”— {prop['source_url']}")
            
            # 4. å„²å­˜æœ¬åœ°æª”æ¡ˆ
            json_file = crawler.save_to_local_file(properties)
            
            # 5. å˜—è©¦ä¸Šå‚³åˆ° Notion
            success = crawler.upload_to_notion_simple(properties, comparison)
            
            # 6. ç¸½çµ
            print(f"\nğŸ“Š {crawler.district_config['name']}å€çˆ¬å–ç¸½çµ:")
            print(f"  â€¢ ä»Šå¤©æ‰¾åˆ°ç‰©ä»¶: {len(properties)} å€‹")
            
            if comparison['has_previous_data']:
                print(f"  â€¢ æ˜¨å¤©ç‰©ä»¶æ•¸é‡: {comparison['previous_count']} å€‹")
                print(f"  â€¢ ğŸ†• æ–°å¢ç‰©ä»¶: {comparison['total_new']} å€‹")
                print(f"  â€¢ ğŸ“¤ ä¸‹æ¶ç‰©ä»¶: {comparison['total_removed']} å€‹")
                print(f"  â€¢ ğŸ’° è®Šåƒ¹ç‰©ä»¶: {comparison['total_price_changed']} å€‹")
                
                # é¡¯ç¤ºæ·¨è®ŠåŒ–
                net_change = comparison.get('change', 0)
                if net_change > 0:
                    print(f"  â€¢ ğŸ“ˆ æ·¨å¢åŠ : +{net_change} å€‹")
                elif net_change < 0:
                    print(f"  â€¢ ğŸ“‰ æ·¨æ¸›å°‘: {abs(net_change)} å€‹")
                else:
                    print(f"  â€¢ â¡ï¸ æ•¸é‡ç„¡è®ŠåŒ–")
            else:
                print(f"  â€¢ ğŸ†• æ–°å¢ç‰©ä»¶: {comparison['total_new']} å€‹ (é¦–æ¬¡åŸ·è¡Œ)")
            
            print(f"  â€¢ ğŸ“ æœ¬åœ°æª”æ¡ˆ: {json_file}")
            print(f"  â€¢ ğŸ”— Notionä¸Šå‚³: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'}")
            
            # é¡¯ç¤ºé‡é»ç‰©ä»¶
            if comparison.get('new_properties'):
                print(f"\nğŸ’¡ ğŸ†• æ–°å¢ç‰©ä»¶é è¦½ (å…± {len(comparison['new_properties'])} å€‹):")
                for i, prop in enumerate(comparison['new_properties'][:3], 1):  # åªé¡¯ç¤ºå‰3å€‹
                    print(f"  {i}. {prop['title'][:30]}... - {prop['price']}è¬å…ƒ")
                    print(f"     ï¿½ {prop['address']}")
                    print(f"     ğŸ”— {prop['source_url']}")
                
                if len(comparison['new_properties']) > 3:
                    print(f"     ... åŠå…¶ä»– {len(comparison['new_properties']) - 3} å€‹")
            
            if comparison.get('price_changed_properties'):
                print(f"\nğŸ’° è®Šåƒ¹ç‰©ä»¶é è¦½ (å…± {len(comparison['price_changed_properties'])} å€‹):")
                for i, change_info in enumerate(comparison['price_changed_properties'][:2], 1):  # åªé¡¯ç¤ºå‰2å€‹
                    prop = change_info['property']
                    old_price = change_info['old_price']
                    new_price = change_info['new_price']
                    change_amount = change_info['change']
                    
                    change_symbol = "ğŸ“ˆ" if change_amount > 0 else "ğŸ“‰"
                    change_text = f"+{change_amount}" if change_amount > 0 else str(change_amount)
                    
                    print(f"  {i}. {prop['title'][:30]}...")
                    print(f"     ğŸ’° {old_price} â†’ {new_price} è¬å…ƒ ({change_symbol} {change_text})")
                    print(f"     ğŸ“ {prop['address']}")
                
                if len(comparison['price_changed_properties']) > 2:
                    print(f"     ... åŠå…¶ä»– {len(comparison['price_changed_properties']) - 2} å€‹")
                
        except KeyboardInterrupt:
            print(f"\nâ¹ï¸  ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
            break
        except Exception as e:
            print(f"\nâŒ {district} å€åŸŸåŸ·è¡ŒéŒ¯èª¤: {str(e)}")
            continue


if __name__ == "__main__":
    main()
