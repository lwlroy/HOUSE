#!/usr/bin/env python3
"""
ä¿¡ç¾©æˆ¿å±‹è˜†æ´²è¯å»ˆå¤§æ¨“ç°¡åŒ–ç‰ˆçˆ¬èŸ²
ä½¿ç”¨ requests å’ŒåŸºæœ¬å¥—ä»¶ï¼Œé¿å…è¤‡é›œä¾è³´
"""

import json
import re
import os
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin
import sys
from pathlib import Path

# å˜—è©¦åŒ¯å…¥å¥—ä»¶
try:
    import requests
    from bs4 import BeautifulSoup
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦å¥—ä»¶: {e}")
    print("è«‹å®‰è£: pip3 install requests beautifulsoup4 --user")
    sys.exit(1)

# åŠ å…¥å°ˆæ¡ˆè·¯å¾‘
sys.path.append(str(Path(__file__).parent))

try:
    from src.models.property import Property
    from src.utils.full_notion import create_full_notion_client
    from src.utils.logger import get_logger
except ImportError as e:
    print(f"âš ï¸  ç„¡æ³•è¼‰å…¥å°ˆæ¡ˆæ¨¡çµ„: {e}")
    print("å°‡ä½¿ç”¨ç°¡åŒ–æ¨¡å¼é‹è¡Œ...")
    Property = None


class SimpleSinyiCrawler:
    """ä¿¡ç¾©æˆ¿å±‹è¯å»ˆå¤§æ¨“ç°¡åŒ–ç‰ˆçˆ¬èŸ²ï¼ˆæ”¯æ´å¤šå€åŸŸï¼‰"""
    
    def __init__(self, district="luzhou"):
        self.base_url = "https://www.sinyi.com.tw"
        
        # å®šç¾©ä¸åŒå€åŸŸçš„æœå°‹é…ç½®
        self.districts = {
            "luzhou": {
                "name": "è˜†æ´²",
                "zip_code": "247",
                "search_url": "https://www.sinyi.com.tw/buy/list/3000-down-price/huaxia-dalou-type/20-up-balconyarea/3-5-roomtotal/NewTaipei-city/247-zip/default-desc",
                "notion_title": "ä¿¡ç¾©è˜†æ´²è¯å»ˆå¤§æ¨“ç‰©ä»¶æ¸…å–®"
            },
            "sanchong": {
                "name": "ä¸‰é‡",
                "zip_code": "241",
                "search_url": "https://www.sinyi.com.tw/buy/list/3000-down-price/huaxia-dalou-type/20-up-balconyarea/3-5-roomtotal/NewTaipei-city/241-zip/default-desc",
                "notion_title": "ä¿¡ç¾©ä¸‰é‡è¯å»ˆå¤§æ¨“ç‰©ä»¶æ¸…å–®"
            },
            "taipei": {
                "name": "å°åŒ—",
                "zip_code": "100-103-104-105-106-108-110-115",
                "search_url": "https://www.sinyi.com.tw/buy/list/3000-down-price/apartment-type/20-up-balconyarea/3-5-roomtotal/1-3-floor/Taipei-city/100-103-104-105-106-108-110-115-zip/default-desc",
                "notion_title": "ä¿¡ç¾©å°åŒ—å…¬å¯“ç‰©ä»¶æ¸…å–®"
            }
        }
        
        # è¨­å®šç•¶å‰å€åŸŸ
        if district not in self.districts:
            raise ValueError(f"ä¸æ”¯æ´çš„å€åŸŸ: {district}ã€‚æ”¯æ´çš„å€åŸŸ: {list(self.districts.keys())}")
        
        self.current_district = district
        self.district_config = self.districts[district]
        self.search_base_url = self.district_config["search_url"]
        
        print(f"ğŸ¯ è¨­å®šçˆ¬èŸ²å€åŸŸ: {self.district_config['name']}å€")
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        # è™•ç† SSL æ†‘è­‰å•é¡Œ
        self.session.verify = False
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs("data", exist_ok=True)
        os.makedirs("logs", exist_ok=True)
    
    def fetch_page(self, url: str, delay: float = 2.0) -> Optional[str]:
        """ç²å–é é¢å…§å®¹"""
        try:
            time.sleep(delay)  # ç¦®è²Œæ€§å»¶é²
            
            print(f"ğŸ” æ­£åœ¨ç²å–: {url}")
            response = self.session.get(url, timeout=30)
            
            if response.status_code == 200:
                print(f"âœ… æˆåŠŸç²å–é é¢ï¼Œå…§å®¹é•·åº¦: {len(response.text)}")
                return response.text
            else:
                print(f"âŒ HTTPéŒ¯èª¤ {response.status_code}: {url}")
                return None
                
        except Exception as e:
            print(f"âŒ ç²å–é é¢å¤±æ•— {url}: {str(e)}")
            return None
    
    def get_total_pages(self) -> int:
        """ç²å–ç¸½é æ•¸"""
        first_page_url = f"{self.search_base_url}/1"
        html = self.fetch_page(first_page_url)
        
        if not html:
            return 1
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # å°‹æ‰¾åˆ†é ç›¸é—œçš„æ–‡å­—
        page_text = soup.get_text()
        
        # å°‹æ‰¾é¡ä¼¼ "ç¬¬ 1 é ï¼Œå…± 5 é " çš„æ–‡å­—
        page_match = re.search(r'å…±\s*(\d+)\s*é ', page_text)
        if page_match:
            total_pages = int(page_match.group(1))
            print(f"ğŸ“„ æª¢æ¸¬åˆ°ç¸½é æ•¸: {total_pages}")
            return total_pages
        
        # å°‹æ‰¾ "ç¬¬Xé " çš„æ¨¡å¼
        page_match2 = re.search(r'ç¬¬\s*\d+\s*é ï¼Œå…±\s*(\d+)\s*é ', page_text)
        if page_match2:
            total_pages = int(page_match2.group(1))
            print(f"ğŸ“„ æª¢æ¸¬åˆ°ç¸½é æ•¸: {total_pages}")
            return total_pages
        
        # å°‹æ‰¾åˆ†é å°èˆªå…ƒç´ 
        pagination_selectors = [
            'ul.pagination a',
            '.pagination a',
            'nav a',
            '.page-numbers a',
            'a[href*="/buy/list/"]'
        ]
        
        max_page = 1
        for selector in pagination_selectors:
            try:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href', '')
                    # å°‹æ‰¾åŒ…å«é ç¢¼çš„URL
                    page_match = re.search(r'/(\d+)$', href)
                    if page_match:
                        page_num = int(page_match.group(1))
                        if 1 <= page_num <= 100:  # åˆç†çš„é æ•¸ç¯„åœ
                            max_page = max(max_page, page_num)
                            
                    # ä¹Ÿæª¢æŸ¥é€£çµæ–‡å­—æ˜¯å¦ç‚ºæ•¸å­—
                    link_text = link.get_text().strip()
                    if link_text.isdigit():
                        page_num = int(link_text)
                        if 1 <= page_num <= 100:
                            max_page = max(max_page, page_num)
            except:
                continue
        
        # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå˜—è©¦é€šéæª¢æŸ¥ä¸‹ä¸€é æ˜¯å¦å­˜åœ¨ä¾†ç¢ºå®šç¸½é æ•¸
        if max_page == 1:
            print("ğŸ“„ å˜—è©¦é€šéæª¢æŸ¥é é¢å­˜åœ¨æ€§ä¾†ç¢ºå®šç¸½é æ•¸...")
            for test_page in range(2, 21):  # æ¸¬è©¦åˆ°ç¬¬20é 
                test_url = f"{self.search_base_url}/{test_page}"
                print(f"   æª¢æŸ¥ç¬¬ {test_page} é ...")
                test_html = self.fetch_page(test_url, delay=1.0)
                
                if test_html:
                    test_soup = BeautifulSoup(test_html, 'html.parser')
                    # æª¢æŸ¥æ˜¯å¦æœ‰ç‰©ä»¶è³‡æ–™
                    property_links = test_soup.find_all('a', href=re.compile(r'/buy/house/'))
                    if property_links:
                        max_page = test_page
                        print(f"   âœ… ç¬¬ {test_page} é æœ‰è³‡æ–™")
                    else:
                        print(f"   âŒ ç¬¬ {test_page} é ç„¡è³‡æ–™ï¼Œåœæ­¢æª¢æŸ¥")
                        break
                else:
                    print(f"   âŒ ç¬¬ {test_page} é ç„¡æ³•å­˜å–ï¼Œåœæ­¢æª¢æŸ¥")
                    break
        
        print(f"ğŸ“„ ç¢ºå®šç¸½é æ•¸: {max_page}")
        return max_page
    
    def parse_property_list(self, html: str) -> List[Dict[str, Any]]:
        """è§£ææˆ¿å±‹åˆ—è¡¨é é¢"""
        properties = []
        soup = BeautifulSoup(html, 'html.parser')
        
        # å°‹æ‰¾åŒ…å« /buy/house/ çš„é€£çµ
        property_links = soup.find_all('a', href=re.compile(r'/buy/house/'))
        
        print(f"ğŸ  æ‰¾åˆ° {len(property_links)} å€‹ç‰©ä»¶é€£çµ")
        
        # å·²è™•ç†çš„ç‰©ä»¶IDï¼Œé¿å…é‡è¤‡
        processed_ids = set()
        
        for link in property_links:
            try:
                href = link.get('href', '')
                if not href:
                    continue
                
                # æå–ç‰©ä»¶ID
                url_match = re.search(r'/buy/house/([A-Za-z0-9]+)', href)
                if not url_match:
                    continue
                
                object_id = url_match.group(1)
                
                # é¿å…é‡è¤‡è™•ç†
                if object_id in processed_ids:
                    continue
                processed_ids.add(object_id)
                
                # æ‰¾åˆ°é€™å€‹é€£çµæ‰€åœ¨çš„å®¹å™¨
                container = link.find_parent(['div', 'article', 'section'])
                if not container:
                    container = link
                
                property_info = self.extract_property_info(container, object_id, href)
                if property_info:
                    properties.append(property_info)
                    print(f"âœ… è§£æç‰©ä»¶: {property_info.get('title', 'Unknown')[:30]}...")
                
            except Exception as e:
                print(f"âš ï¸  è§£æç‰©ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue
        
        return properties
    
    def extract_property_info(self, container, object_id: str, href: str) -> Optional[Dict[str, Any]]:
        """å¾å®¹å™¨ä¸­æå–æˆ¿å±‹è³‡è¨Š"""
        
        detail_url = urljoin(self.base_url, href)
        
        # å–å¾—å®¹å™¨å…§çš„æ‰€æœ‰æ–‡å­—
        container_text = container.get_text()
        
        # æå–æ¨™é¡Œ - é€šå¸¸æ˜¯é€£çµçš„æ–‡å­—
        title_elem = container.find('a', href=re.compile(r'/buy/house/'))
        title = self.clean_text(title_elem.get_text()) if title_elem else "æœªçŸ¥ç‰©ä»¶"
        
        # å¦‚æœæ¨™é¡Œå¤ªçŸ­ï¼Œå˜—è©¦å°‹æ‰¾æ›´è©³ç´°çš„æ¨™é¡Œ
        if len(title) < 5:
            title_candidates = container.find_all(['h1', 'h2', 'h3', 'h4', 'h5'])
            for candidate in title_candidates:
                candidate_text = self.clean_text(candidate.get_text())
                if len(candidate_text) > len(title):
                    title = candidate_text
                    break
        
        # æå–åƒ¹æ ¼
        price = self.extract_price(container_text)
        
        # æå–åœ°å€
        address = self.extract_address(container_text)
        
        # æå–æˆ¿å‹è³‡è¨Š
        room_info = self.extract_room_info(container_text)
        
        # æå–åªæ•¸
        size_info = self.extract_size_info(container_text)
        
        # æå–æ¨“å±¤
        floor_info = self.extract_floor_info(container_text)
        
        # æå–å±‹é½¡
        age_info = self.extract_age_info(container_text)
        
        property_info = {
            'id': f"sinyi_{self.current_district}_{object_id}",
            'object_id': object_id,
            'title': title,
            'address': address,
            'district': f'{self.district_config["name"]}å€',
            'region': 'æ–°åŒ—å¸‚',
            'price': price,
            'total_price': price,
            'room_count': room_info.get('rooms', 3),
            'living_room_count': room_info.get('living_rooms', 2),
            'bathroom_count': room_info.get('bathrooms', 2),
            'size': size_info.get('total_size', 0),
            'main_area': size_info.get('main_area', 0),
            'floor': floor_info,
            'age': age_info,
            'building_type': 'è¯å»ˆ/å¤§æ¨“',
            'source_site': 'ä¿¡ç¾©æˆ¿å±‹',
            'source_url': detail_url,
            'property_type': 'sale',
            'features': [],
            'equipment': [],
            'image_urls': [],
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        return property_info
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡å­—"""
        if not text:
            return ""
        return re.sub(r'\s+', ' ', str(text)).strip()
    
    def extract_price(self, text: str) -> float:
        """æå–åƒ¹æ ¼ï¼ˆè¬å…ƒï¼‰"""
        # å°‹æ‰¾åƒ¹æ ¼æ¨¡å¼
        price_patterns = [
            r'(\d{1,4}(?:,\d{3})*(?:\.\d+)?)\s*è¬',
            r'ç¸½åƒ¹[ï¼š:\s]*(\d{1,4}(?:,\d{3})*(?:\.\d+)?)',
            r'(\d{3,4})\s*è¬'
        ]
        
        for pattern in price_patterns:
            price_match = re.search(pattern, text)
            if price_match:
                price_str = price_match.group(1).replace(',', '')
                try:
                    return float(price_str)
                except:
                    continue
        
        return 0
    
    def extract_address(self, text: str) -> str:
        """æå–åœ°å€"""
        district_name = self.district_config["name"]
        
        # å°‹æ‰¾åŒ…å«è©²å€åŸŸçš„åœ°å€
        address_patterns = [
            f'(æ–°åŒ—å¸‚{district_name}å€[^ï¼Œ\\n]{{1,30}})',
            f'({district_name}å€[^ï¼Œ\\n]{{1,30}})',
            f'æ–°åŒ—å¸‚\\s*{district_name}å€\\s*([^ï¼Œ\\n]{{1,20}})'
        ]
        
        for pattern in address_patterns:
            match = re.search(pattern, text)
            if match:
                return self.clean_text(match.group(1))
        
        return f"{district_name}å€"
    
    def extract_room_info(self, text: str) -> Dict[str, int]:
        """æå–æˆ¿å‹è³‡è¨Š"""
        # å„ªå…ˆå°‹æ‰¾æ¨™æº–æ ¼å¼
        room_match = re.search(r'(\d+)æˆ¿(\d+)å»³(\d+)è¡›', text)
        if room_match:
            rooms = int(room_match.group(1))
            living_rooms = int(room_match.group(2))
            bathrooms = int(room_match.group(3))
            
            # åˆç†æ€§æª¢æŸ¥
            if 1 <= rooms <= 10 and 1 <= living_rooms <= 5 and 1 <= bathrooms <= 5:
                return {
                    'rooms': rooms,
                    'living_rooms': living_rooms,
                    'bathrooms': bathrooms
                }
        
        # å°‹æ‰¾å…¶ä»–æ ¼å¼
        room_match2 = re.search(r'(\d+)R(\d+)L(\d+)B', text)
        if room_match2:
            rooms = int(room_match2.group(1))
            living_rooms = int(room_match2.group(2))
            bathrooms = int(room_match2.group(3))
            
            if 1 <= rooms <= 10 and 1 <= living_rooms <= 5 and 1 <= bathrooms <= 5:
                return {
                    'rooms': rooms,
                    'living_rooms': living_rooms,
                    'bathrooms': bathrooms
                }
        
        # åªå°‹æ‰¾æˆ¿é–“æ•¸
        room_only_match = re.search(r'(\d+)\s*æˆ¿', text)
        if room_only_match:
            rooms = int(room_only_match.group(1))
            if 1 <= rooms <= 10:
                return {
                    'rooms': rooms,
                    'living_rooms': 2,  # é è¨­å€¼
                    'bathrooms': 2      # é è¨­å€¼
                }
        
        # é è¨­å€¼
        return {'rooms': 3, 'living_rooms': 2, 'bathrooms': 2}
    
    def extract_size_info(self, text: str) -> Dict[str, float]:
        """æå–åªæ•¸è³‡è¨Š"""
        # å»ºåª
        size_patterns = [
            r'å»ºåª[ï¼š:\s]*(\d+(?:\.\d+)?)',
            r'ç¸½åªæ•¸[ï¼š:\s]*(\d+(?:\.\d+)?)',
            r'(\d+(?:\.\d+)?)\s*åª'
        ]
        
        total_size = 0
        for pattern in size_patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    total_size = float(match.group(1))
                    break
                except:
                    continue
        
        # ä¸»å»ºç‰©
        main_match = re.search(r'ä¸»å»ºç‰©[ï¼š:\s]*(\d+(?:\.\d+)?)', text)
        main_area = float(main_match.group(1)) if main_match else total_size * 0.8
        
        return {
            'total_size': total_size,
            'main_area': main_area
        }
    
    def extract_floor_info(self, text: str) -> str:
        """æå–æ¨“å±¤è³‡è¨Š"""
        floor_patterns = [
            r'(\d+)æ¨“/(\d+)æ¨“',
            r'(\d+)F/(\d+)F',
            r'(\d+)æ¨“',
            r'(\d+)F'
        ]
        
        for pattern in floor_patterns:
            floor_match = re.search(pattern, text)
            if floor_match:
                if len(floor_match.groups()) >= 2 and floor_match.group(2):
                    return f"{floor_match.group(1)}æ¨“/{floor_match.group(2)}æ¨“"
                else:
                    return f"{floor_match.group(1)}æ¨“"
        
        return "æœªçŸ¥æ¨“å±¤"
    
    def extract_age_info(self, text: str) -> int:
        """æå–å±‹é½¡"""
        age_patterns = [
            r'å±‹é½¡[ï¼š:\s]*(\d+(?:\.\d+)?)\s*å¹´',
            r'(\d+(?:\.\d+)?)\s*å¹´å±‹',
            r'æ°‘åœ‹\s*(\d+)\s*å¹´'
        ]
        
        for pattern in age_patterns:
            age_match = re.search(pattern, text)
            if age_match:
                try:
                    age = float(age_match.group(1))
                    # å¦‚æœæ˜¯æ°‘åœ‹å¹´ä»½ï¼Œè½‰æ›ç‚ºå±‹é½¡
                    if pattern.startswith(r'æ°‘åœ‹') and age > 50:
                        current_year = datetime.now().year - 1911  # æ°‘åœ‹å¹´
                        age = current_year - age
                    return int(age)
                except:
                    continue
        
        return 0
    
    def crawl_all_pages(self, max_pages: int = None) -> List[Dict[str, Any]]:
        """çˆ¬å–æ‰€æœ‰é é¢çš„ç‰©ä»¶"""
        district_name = self.district_config["name"]
        print(f"ğŸ” é–‹å§‹çˆ¬å–ä¿¡ç¾©æˆ¿å±‹{district_name}è¯å»ˆå¤§æ¨“ç‰©ä»¶...")
        
        # ç²å–ç¸½é æ•¸
        detected_total_pages = self.get_total_pages()
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§é æ•¸é™åˆ¶ï¼Œå‰‡ä½¿ç”¨è¼ƒå°å€¼
        if max_pages is not None:
            total_pages = min(detected_total_pages, max_pages)
            print(f"ğŸ“„ æª¢æ¸¬åˆ° {detected_total_pages} é ï¼Œä½†é™åˆ¶ç‚º {max_pages} é ï¼Œå°‡çˆ¬å– {total_pages} é ")
        else:
            total_pages = detected_total_pages
            print(f"ğŸ“„ å°‡çˆ¬å–æ‰€æœ‰ {total_pages} é ")
        
        all_properties = []
        
        # çˆ¬å–æ¯ä¸€é 
        for page in range(1, total_pages + 1):
            page_url = f"{self.search_base_url}/{page}"
            print(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page}/{total_pages} é ...")
            
            html = self.fetch_page(page_url, delay=2.0)  # é©ç•¶å»¶é²é¿å…è¢«å°
            
            if html:
                page_properties = self.parse_property_list(html)
                
                # å¦‚æœç•¶å‰é é¢æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç‰©ä»¶ï¼Œå¯èƒ½æ˜¯åˆ°äº†æœ€å¾Œä¸€é 
                if not page_properties:
                    print(f"âš ï¸  ç¬¬ {page} é æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç‰©ä»¶ï¼Œå¯èƒ½å·²åˆ°é”æœ€å¾Œä¸€é ")
                    break
                
                all_properties.extend(page_properties)
                print(f"âœ… ç¬¬ {page} é æ‰¾åˆ° {len(page_properties)} å€‹ç‰©ä»¶")
                
                # æª¢æŸ¥æ˜¯å¦æ‰¾åˆ°é‡è¤‡çš„ç‰©ä»¶IDï¼ˆè¡¨ç¤ºå¯èƒ½å¾ªç’°åˆ°å·²çˆ¬éçš„é é¢ï¼‰
                if page > 1:
                    current_ids = {prop['object_id'] for prop in page_properties}
                    previous_ids = {prop['object_id'] for prop in all_properties[:-len(page_properties)]}
                    duplicate_ids = current_ids.intersection(previous_ids)
                    
                    if duplicate_ids:
                        print(f"âš ï¸  ç¬¬ {page} é ç™¼ç¾é‡è¤‡ç‰©ä»¶ï¼Œå¯èƒ½å·²åˆ°é”å¯¦éš›æœ€å¾Œä¸€é ")
                        # ç§»é™¤é‡è¤‡çš„ç‰©ä»¶
                        all_properties = all_properties[:-len(page_properties)]
                        break
                        
            else:
                print(f"âŒ ç¬¬ {page} é çˆ¬å–å¤±æ•—")
                if page > 1:  # å¦‚æœä¸æ˜¯ç¬¬ä¸€é å°±å¤±æ•—ï¼Œå¯èƒ½æ˜¯åˆ°äº†æœ€å¾Œ
                    print(f"âš ï¸  å¯èƒ½å·²åˆ°é”æœ€å¾Œä¸€é ")
                    break
        
        # å»é™¤é‡è¤‡ç‰©ä»¶ï¼ˆä»¥é˜²è¬ä¸€ï¼‰
        unique_properties = []
        seen_ids = set()
        for prop in all_properties:
            if prop['object_id'] not in seen_ids:
                unique_properties.append(prop)
                seen_ids.add(prop['object_id'])
        
        if len(unique_properties) != len(all_properties):
            print(f"âš ï¸  ç§»é™¤äº† {len(all_properties) - len(unique_properties)} å€‹é‡è¤‡ç‰©ä»¶")
        
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼ç¸½å…±æ‰¾åˆ° {len(unique_properties)} å€‹å”¯ä¸€ç‰©ä»¶")
        return unique_properties
    
    def save_to_local_file(self, properties: List[Dict[str, Any]], filename_prefix: str = None) -> str:
        """å„²å­˜åˆ°æœ¬åœ°æª”æ¡ˆ"""
        if filename_prefix is None:
            filename_prefix = f"{self.current_district}_houses"
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        json_filename = f"data/{filename_prefix}_{timestamp}.json"
        
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(properties, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“ å·²å„²å­˜åˆ°: {json_filename}")
        return json_filename
    
    def load_previous_data(self) -> List[Dict[str, Any]]:
        """è¼‰å…¥å‰ä¸€å¤©çš„è³‡æ–™ç”¨æ–¼æ¯”è¼ƒ"""
        yesterday = datetime.now() - timedelta(days=1)
        yesterday_str = yesterday.strftime('%Y%m%d')
        
        # å°‹æ‰¾æ˜¨å¤©çš„æª”æ¡ˆ
        data_dir = "data"
        if not os.path.exists(data_dir):
            return []
        
        filename_prefix = f"{self.current_district}_houses"
        for filename in os.listdir(data_dir):
            if filename.startswith(f"{filename_prefix}_{yesterday_str}") and filename.endswith('.json'):
                filepath = os.path.join(data_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        print(f"ğŸ“‚ è¼‰å…¥æ˜¨å¤©çš„è³‡æ–™: {len(data)} å€‹ç‰©ä»¶")
                        return data
                except Exception as e:
                    print(f"âŒ è¼‰å…¥æ˜¨å¤©è³‡æ–™å¤±æ•—: {str(e)}")
        
        print("ğŸ“‚ æœªæ‰¾åˆ°æ˜¨å¤©çš„è³‡æ–™")
        return []
    
    def compare_with_previous(self, current_properties: List[Dict[str, Any]], previous_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """èˆ‡å‰ä¸€å¤©çš„è³‡æ–™æ¯”è¼ƒ"""
        if not previous_data:
            return {
                'new_properties': current_properties,
                'total_new': len(current_properties),
                'message': 'é¦–æ¬¡çˆ¬å–ï¼Œæ‰€æœ‰ç‰©ä»¶éƒ½æ˜¯æ–°çš„'
            }
        
        # å»ºç«‹æ˜¨å¤©çš„ç‰©ä»¶IDé›†åˆ
        previous_ids = {prop.get('object_id', prop.get('id', '')) for prop in previous_data}
        
        # æ‰¾å‡ºæ–°å¢çš„ç‰©ä»¶
        new_properties = []
        for prop in current_properties:
            if prop['object_id'] not in previous_ids:
                new_properties.append(prop)
        
        return {
            'new_properties': new_properties,
            'total_new': len(new_properties),
            'total_current': len(current_properties),
            'total_previous': len(previous_data),
            'message': f'èˆ‡æ˜¨å¤©æ¯”è¼ƒï¼šæ–°å¢ {len(new_properties)} å€‹ç‰©ä»¶'
        }
    
    def upload_to_notion_simple(self, properties: List[Dict[str, Any]], comparison_data: Dict = None) -> bool:
        """å˜—è©¦ä¸Šå‚³åˆ° Notionï¼ˆæ–°å±¤ç´šçµæ§‹ï¼‰"""
        notion_token = os.getenv('NOTION_API_TOKEN', os.getenv('NOTION_TOKEN'))
        
        if not notion_token:
            print("âš ï¸  æœªè¨­å®š NOTION_API_TOKENï¼Œè·³é Notion ä¸Šå‚³")
            print("ğŸ’¡ è«‹è¨­å®šç’°å¢ƒè®Šæ•¸æˆ–åŸ·è¡Œ setup_notion.py")
            return False
        
        try:
            # å˜—è©¦åŒ¯å…¥ Notion åŠŸèƒ½
            from src.utils.full_notion import create_full_notion_client
            from src.models.property import Property
            
            print("ğŸ”„ æ­£åœ¨ä¸Šå‚³åˆ° Notion...")
            
            client = create_full_notion_client(notion_token)
            
            if not client.test_connection():
                print("âŒ Notion API é€£æ¥å¤±æ•—")
                return False
            
            # å°‡å­—å…¸è³‡æ–™è½‰æ›ç‚º Property å°è±¡
            property_objects = []
            for prop_dict in properties:
                try:
                    prop = Property(
                        title=prop_dict.get('title', ''),
                        address=prop_dict.get('address', ''),
                        total_price=prop_dict.get('price', 0),
                        size=prop_dict.get('size', 0),
                        source_url=prop_dict.get('url', ''),
                        source_site="ä¿¡ç¾©æˆ¿å±‹"
                    )
                    
                    # è¨­å®šå…¶ä»–å±¬æ€§ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                    if 'main_area' in prop_dict:
                        prop.main_area = prop_dict['main_area']
                    if 'unit_price' in prop_dict:
                        prop.unit_price = prop_dict['unit_price']
                    if 'room_count' in prop_dict:
                        prop.room_count = prop_dict['room_count']
                    if 'living_room_count' in prop_dict:
                        prop.living_room_count = prop_dict['living_room_count']
                    if 'bathroom_count' in prop_dict:
                        prop.bathroom_count = prop_dict['bathroom_count']
                    if 'floor' in prop_dict:
                        prop.floor = prop_dict['floor']
                    if 'total_floors' in prop_dict:
                        prop.total_floors = prop_dict['total_floors']
                    if 'age' in prop_dict:
                        prop.age = prop_dict['age']
                    if 'building_type' in prop_dict:
                        prop.building_type = prop_dict['building_type']
                    
                    property_objects.append(prop)
                except Exception as e:
                    print(f"âš ï¸  è½‰æ›ç‰©ä»¶å¤±æ•—: {e}")
                    continue
            
            # ä½¿ç”¨æ–°çš„å±¤ç´šçµæ§‹ä¸Šå‚³
            today = datetime.now()
            district_name = self._get_district_key()  # ç²å–è‹±æ–‡å€åŸŸåç¨±
            
            # ä½¿ç”¨æ–°çš„å€åŸŸç‰©ä»¶æ¸…å–®å‰µå»ºæ–¹æ³•
            success = client.create_district_house_list(
                properties=property_objects,
                search_date=today,
                district_name=district_name,
                comparison=None  # æš«æ™‚ä¸æä¾›æ¯”è¼ƒè³‡æ–™
            )
            
            if success:
                print("âœ… æˆåŠŸä¸Šå‚³åˆ° Notionï¼")
                return True
            else:
                print("âŒ Notion ä¸Šå‚³å¤±æ•—")
                return False
                
        except ImportError:
            print("âš ï¸  ç„¡æ³•è¼‰å…¥ Notion åŠŸèƒ½ï¼Œè·³éä¸Šå‚³")
            return False
        except Exception as e:
            print(f"âŒ Notion ä¸Šå‚³éŒ¯èª¤: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def _get_district_key(self) -> str:
        """ç²å–å€åŸŸçš„è‹±æ–‡éµå€¼"""
        district_map = {
            'è˜†æ´²': 'luzhou',
            'ä¸‰é‡': 'sanchong',
            'å°åŒ—': 'taipei'
        }
        return district_map.get(self.district_config['name'], 'unknown')

def main():
    """ä¸»ç¨‹å¼"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¿¡ç¾©æˆ¿å±‹è¯å»ˆå¤§æ¨“ç‰©ä»¶çˆ¬èŸ²')
    parser.add_argument('--district', 
                       choices=['luzhou', 'sanchong', 'taipei', 'all'], 
                       default='luzhou',
                       help='é¸æ“‡çˆ¬å–å€åŸŸ: luzhou(è˜†æ´²), sanchong(ä¸‰é‡), taipei(å°åŒ—), all(å…¨éƒ¨)')
    
    args = parser.parse_args()
    
    if args.district == 'all':
        # çˆ¬å–æ‰€æœ‰å€åŸŸ
        districts_to_crawl = ['luzhou', 'sanchong', 'taipei']
    else:
        districts_to_crawl = [args.district]
    
    print("ğŸ  ä¿¡ç¾©æˆ¿å±‹è¯å»ˆå¤§æ¨“ç‰©ä»¶çˆ¬èŸ²")
    print("=" * 50)
    
    for district in districts_to_crawl:
        print(f"\nğŸ¯ é–‹å§‹çˆ¬å– {district} å€åŸŸ...")
        print("-" * 30)
        
        try:
            crawler = SimpleSinyiCrawler(district=district)
            
            # 1. è¼‰å…¥å‰ä¸€å¤©çš„è³‡æ–™
            print("ğŸ“‚ è¼‰å…¥å‰ä¸€å¤©çš„è³‡æ–™...")
            previous_data = crawler.load_previous_data()
            
            # 2. çˆ¬å–ä»Šå¤©çš„è³‡æ–™ï¼ˆä¸é™åˆ¶é æ•¸ï¼Œçˆ¬å–æ‰€æœ‰é é¢ï¼‰
            properties = crawler.crawl_all_pages()  # ç§»é™¤ max_pages é™åˆ¶
            
            if not properties:
                print("âŒ æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•è³‡æ–™")
                continue
            
            # 3. èˆ‡å‰ä¸€å¤©æ¯”è¼ƒ
            comparison = crawler.compare_with_previous(properties, previous_data)
            print(f"ğŸ“ˆ {comparison['message']}")
            
            if comparison['new_properties']:
                print("\nğŸ†• æ–°å¢çš„ç‰©ä»¶:")
                for i, prop in enumerate(comparison['new_properties'][:5], 1):  # åªé¡¯ç¤ºå‰5å€‹
                    print(f"  {i}. {prop['title'][:30]}... - {prop['price']}è¬å…ƒ")
                    print(f"     ğŸ”— {prop['source_url']}")
            
            # 4. å„²å­˜æœ¬åœ°æª”æ¡ˆ
            json_file = crawler.save_to_local_file(properties)
            
            # 5. å˜—è©¦ä¸Šå‚³åˆ° Notion
            success = crawler.upload_to_notion_simple(properties, comparison)
            
            # 6. ç¸½çµ
            print(f"\nğŸ“Š {crawler.district_config['name']}å€çˆ¬å–ç¸½çµ:")
            print(f"  â€¢ ä»Šå¤©æ‰¾åˆ°ç‰©ä»¶: {len(properties)} å€‹")
            print(f"  â€¢ æ–°å¢ç‰©ä»¶: {comparison['total_new']} å€‹")
            print(f"  â€¢ æœ¬åœ°æª”æ¡ˆ: {json_file}")
            print(f"  â€¢ Notionä¸Šå‚³: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'}")
            
            if comparison['new_properties']:
                print(f"\nğŸ’¡ ç™¼ç¾ {len(comparison['new_properties'])} å€‹æ–°ç‰©ä»¶ï¼")
                
        except KeyboardInterrupt:
            print(f"\nâ¹ï¸  ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
            break
        except Exception as e:
            print(f"\nâŒ {district} å€åŸŸåŸ·è¡ŒéŒ¯èª¤: {str(e)}")
            continue


if __name__ == "__main__":
    main()
