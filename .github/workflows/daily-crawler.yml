name: Daily House Crawler

on:
  schedule:
    # æ¯å¤©å°ç£æ™‚é–“æ—©ä¸Š 9:00 åŸ·è¡Œ (UTC+8, æ‰€ä»¥ UTC æ™‚é–“æ˜¯ 01:00)
    - cron: '0 1 * * *'
  workflow_dispatch: # å…è¨±æ‰‹å‹•è§¸ç™¼
  push:
    branches: [ main ]  # ç•¶æ¨é€åˆ° main åˆ†æ”¯æ™‚ä¹Ÿæœƒè§¸ç™¼ï¼ˆåƒ…ç”¨æ–¼æ¸¬è©¦ï¼‰

jobs:
  crawl-houses:
    runs-on: ubuntu-latest
    permissions:
      actions: read  # éœ€è¦è®€å– actions çš„æ¬Šé™ä¾†ä¸‹è¼‰ artifacts
      contents: read
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Download previous day data
      id: download-artifact
      uses: dawidd6/action-download-artifact@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        workflow: daily-crawler.yml
        name: house-data
        path: ./previous_data/
        if_no_artifact_found: ignore
        search_artifacts: true
        skip_unpack: false
      continue-on-error: true
      
    - name: Download previous day data (backup method)
      if: steps.download-artifact.outcome == 'failure'
      run: |
        echo "ğŸ”„ å˜—è©¦ä½¿ç”¨ GitHub CLI ä¸‹è¼‰å‰ä¸€å¤©çš„ artifacts..."
        
        # ä½¿ç”¨ GitHub CLI åˆ—å‡ºæœ€è¿‘çš„ workflow runs
        echo "ğŸ” å°‹æ‰¾å‰ä¸€æ¬¡æˆåŠŸçš„ workflow run..."
        PREV_RUN_ID=$(gh run list --workflow=daily-crawler.yml --status=success --limit=5 --json databaseId --jq '.[0].databaseId' 2>/dev/null || echo "")
        
        if [ -n "$PREV_RUN_ID" ] && [ "$PREV_RUN_ID" != "null" ]; then
          echo "âœ… æ‰¾åˆ°å‰ä¸€æ¬¡æˆåŠŸåŸ·è¡Œ: $PREV_RUN_ID"
          
          # å˜—è©¦ä¸‹è¼‰ artifacts
          mkdir -p ./previous_data
          if gh run download $PREV_RUN_ID --name house-data --dir ./previous_data 2>/dev/null; then
            echo "âœ… æˆåŠŸä½¿ç”¨ GitHub CLI ä¸‹è¼‰å‰ä¸€å¤©è³‡æ–™"
            ls -la ./previous_data/
          else
            echo "âŒ GitHub CLI ä¸‹è¼‰å¤±æ•—"
          fi
        else
          echo "âš ï¸ æ²’æœ‰æ‰¾åˆ°å‰ä¸€æ¬¡æˆåŠŸçš„ workflow runï¼ˆå¯èƒ½æ˜¯é¦–æ¬¡åŸ·è¡Œï¼‰"
        fi
      env:
        GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      continue-on-error: true
      
    - name: Debug download result
      run: |
        echo "ğŸ” æª¢æŸ¥ä¸‹è¼‰çµæœå’Œæª”æ¡ˆç³»çµ±ç‹€æ…‹ï¼š"
        echo "ğŸ“‚ ç•¶å‰å·¥ä½œç›®éŒ„ï¼š"
        pwd
        ls -la
        echo ""
        echo "ğŸ¯ æª¢æŸ¥ previous_data ç›®éŒ„ï¼š"
        if [ -d "./previous_data" ]; then
          echo "âœ… ./previous_data ç›®éŒ„å­˜åœ¨"
          echo "ğŸ“„ ç›®éŒ„å…§å®¹ï¼š"
          ls -la ./previous_data/
          echo ""
          echo "ğŸ” å°‹æ‰¾ JSON æª”æ¡ˆï¼š"
          find ./previous_data -name "*.json" -type f 2>/dev/null || echo "âŒ æ²’æœ‰æ‰¾åˆ° JSON æª”æ¡ˆ"
          echo ""
          echo "ğŸ“Š JSON æª”æ¡ˆè©³ç´°è³‡è¨Šï¼š"
          find ./previous_data -name "*.json" -type f -exec echo "ğŸ“‹ æª”æ¡ˆ: {}" \; -exec wc -l {} \; 2>/dev/null || echo "âŒ ç„¡æ³•åˆ†æ JSON æª”æ¡ˆ"
        else
          echo "âŒ ./previous_data ç›®éŒ„ä¸å­˜åœ¨"
        fi
    
    - name: Run crawler
      env:
        NOTION_API_TOKEN: ${{ secrets.NOTION_API_TOKEN }}
      run: |
        python simple_luzhou_crawler.py --district all
    
    - name: List current data (for debugging)
      run: |
        if [ -d "./data" ]; then
          echo "ğŸ“ Current data generated:"
          ls -la ./data/
        fi
    
    - name: Upload current data as artifact
      uses: actions/upload-artifact@v4
      with:
        name: house-data
        path: ./data/
        retention-days: 3  # ä¿ç•™ 3 å¤©çš„è³‡æ–™ï¼Œè¶³å¤ åšå‰ä¸€å¤©æ¯”è¼ƒ
    
    - name: Show execution summary
      run: |
        echo "ğŸ‰ House crawler execution completed!"
        echo "ğŸ“Š Check Notion for updated house listings"
        if [ -d "logs" ]; then
          echo "ğŸ“ Logs available in logs/ directory"
        fi
