#!/usr/bin/env python3
"""
ä¿¡ç¾©æˆ¿å±‹ä¸‰é‡è˜†æ´²æ•´åˆç‰ˆçˆ¬èŸ²
ä½¿ç”¨æŒ‡å®šç¶²å€çˆ¬å–ä¸‰é‡è˜†æ´²çš„è¯å»ˆå¤§æ¨“ç‰©ä»¶
"""

import json
import re
import os
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin
import sys
from pathlib import Path

# å˜—è©¦åŒ¯å…¥å¥—ä»¶
try:
    import requests
    from bs4 import BeautifulSoup
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦å¥—ä»¶: {e}")
    print("è«‹å®‰è£: pip3 install requests beautifulsoup4 --user")
    sys.exit(1)

# åŠ å…¥å°ˆæ¡ˆè·¯å¾‘
sys.path.append(str(Path(__file__).parent))

try:
    from src.models.property import Property
    from src.utils.full_notion import create_full_notion_client
    from src.utils.logger import get_logger
except ImportError as e:
    print(f"âš ï¸  ç„¡æ³•è¼‰å…¥å°ˆæ¡ˆæ¨¡çµ„: {e}")
    print("å°‡ä½¿ç”¨ç°¡åŒ–æ¨¡å¼é‹è¡Œ...")
    Property = None


class SanchongLuzhouCrawler:
    """ä¿¡ç¾©æˆ¿å±‹ä¸‰é‡è˜†æ´²æ•´åˆç‰ˆçˆ¬èŸ²"""
    
    def __init__(self):
        self.base_url = "https://www.sinyi.com.tw"
        
        # ä½¿ç”¨æŒ‡å®šçš„æœå°‹URL
        self.search_base_url = "https://www.sinyi.com.tw/buy/list/3000-down-price/dalou-huaxia-type/20-up-balconyarea/3-5-roomtotal/NewTaipei-city/241-247-zip/default-desc"
        
        self.district_name = "ä¸‰é‡è˜†æ´²"
        self.region_name = "æ–°åŒ—å¸‚"
        
        print(f"ğŸ¯ è¨­å®šçˆ¬èŸ²å€åŸŸ: {self.district_name}å€ (ä¸‰é‡+è˜†æ´²)")
        print(f"ğŸ”— æœå°‹ç¶²å€: {self.search_base_url}")
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        # è™•ç† SSL æ†‘è­‰å•é¡Œ
        self.session.verify = False
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs("data", exist_ok=True)
        os.makedirs("logs", exist_ok=True)
    
    def fetch_page(self, url: str, delay: float = 2.0) -> Optional[str]:
        """ç²å–é é¢å…§å®¹"""
        try:
            time.sleep(delay)  # ç¦®è²Œæ€§å»¶é²
            
            print(f"ğŸ” æ­£åœ¨ç²å–: {url}")
            response = self.session.get(url, timeout=30)
            
            if response.status_code == 200:
                print(f"âœ… æˆåŠŸç²å–é é¢ï¼Œå…§å®¹é•·åº¦: {len(response.text)}")
                return response.text
            else:
                print(f"âŒ HTTPéŒ¯èª¤ {response.status_code}: {url}")
                return None
                
        except Exception as e:
            print(f"âŒ ç²å–é é¢å¤±æ•— {url}: {str(e)}")
            return None
    
    def get_total_pages(self) -> int:
        """ç²å–ç¸½é æ•¸"""
        first_page_url = f"{self.search_base_url}/1"
        html = self.fetch_page(first_page_url)
        
        if not html:
            return 1
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # å°‹æ‰¾åˆ†é ç›¸é—œçš„æ–‡å­—
        page_text = soup.get_text()
        
        # å°‹æ‰¾é¡ä¼¼ "ç¬¬ 1 é ï¼Œå…± 5 é " çš„æ–‡å­—
        page_match = re.search(r'å…±\s*(\d+)\s*é ', page_text)
        if page_match:
            total_pages = int(page_match.group(1))
            print(f"ğŸ“„ æª¢æ¸¬åˆ°ç¸½é æ•¸: {total_pages}")
            return total_pages
        
        # å°‹æ‰¾ "ç¬¬Xé " çš„æ¨¡å¼
        page_match2 = re.search(r'ç¬¬\s*\d+\s*é ï¼Œå…±\s*(\d+)\s*é ', page_text)
        if page_match2:
            total_pages = int(page_match2.group(1))
            print(f"ğŸ“„ æª¢æ¸¬åˆ°ç¸½é æ•¸: {total_pages}")
            return total_pages
        
        # å°‹æ‰¾åˆ†é å°èˆªå…ƒç´ 
        pagination_selectors = [
            'ul.pagination a',
            '.pagination a',
            'nav a',
            '.page-numbers a',
            'a[href*="/buy/list/"]'
        ]
        
        max_page = 1
        for selector in pagination_selectors:
            try:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href', '')
                    # å°‹æ‰¾åŒ…å«é ç¢¼çš„URL
                    page_match = re.search(r'/(\d+)$', href)
                    if page_match:
                        page_num = int(page_match.group(1))
                        if 1 <= page_num <= 100:  # åˆç†çš„é æ•¸ç¯„åœ
                            max_page = max(max_page, page_num)
                            
                    # ä¹Ÿæª¢æŸ¥é€£çµæ–‡å­—æ˜¯å¦ç‚ºæ•¸å­—
                    link_text = link.get_text().strip()
                    if link_text.isdigit():
                        page_num = int(link_text)
                        if 1 <= page_num <= 100:
                            max_page = max(max_page, page_num)
            except:
                continue
        
        # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå˜—è©¦é€šéæª¢æŸ¥ä¸‹ä¸€é æ˜¯å¦å­˜åœ¨ä¾†ç¢ºå®šç¸½é æ•¸
        if max_page == 1:
            print("ğŸ“„ å˜—è©¦é€šéæª¢æŸ¥é é¢å­˜åœ¨æ€§ä¾†ç¢ºå®šç¸½é æ•¸...")
            for test_page in range(2, 21):  # æ¸¬è©¦åˆ°ç¬¬20é 
                test_url = f"{self.search_base_url}/{test_page}"
                print(f"   æª¢æŸ¥ç¬¬ {test_page} é ...")
                test_html = self.fetch_page(test_url, delay=1.0)
                
                if test_html:
                    test_soup = BeautifulSoup(test_html, 'html.parser')
                    # æª¢æŸ¥æ˜¯å¦æœ‰ç‰©ä»¶è³‡æ–™
                    property_links = test_soup.find_all('a', href=re.compile(r'/buy/house/'))
                    if property_links:
                        max_page = test_page
                        print(f"   âœ… ç¬¬ {test_page} é æœ‰è³‡æ–™")
                    else:
                        print(f"   âŒ ç¬¬ {test_page} é ç„¡è³‡æ–™ï¼Œåœæ­¢æª¢æŸ¥")
                        break
                else:
                    print(f"   âŒ ç¬¬ {test_page} é ç„¡æ³•å­˜å–ï¼Œåœæ­¢æª¢æŸ¥")
                    break
        
        print(f"ğŸ“„ ç¢ºå®šç¸½é æ•¸: {max_page}")
        return max_page
    
    def parse_property_list(self, html: str) -> List[Dict[str, Any]]:
        """è§£ææˆ¿å±‹åˆ—è¡¨é é¢"""
        properties = []
        soup = BeautifulSoup(html, 'html.parser')
        
        # å°‹æ‰¾åŒ…å« /buy/house/ çš„é€£çµ
        property_links = soup.find_all('a', href=re.compile(r'/buy/house/'))
        
        print(f"ğŸ  æ‰¾åˆ° {len(property_links)} å€‹ç‰©ä»¶é€£çµ")
        
        # å·²è™•ç†çš„ç‰©ä»¶IDï¼Œé¿å…é‡è¤‡
        processed_ids = set()
        
        for link in property_links:
            try:
                href = link.get('href', '')
                if not href:
                    continue
                
                # æå–ç‰©ä»¶ID
                url_match = re.search(r'/buy/house/([A-Za-z0-9]+)', href)
                if not url_match:
                    continue
                
                object_id = url_match.group(1)
                
                # é¿å…é‡è¤‡è™•ç†
                if object_id in processed_ids:
                    continue
                processed_ids.add(object_id)
                
                # æ‰¾åˆ°é€™å€‹é€£çµæ‰€åœ¨çš„å®¹å™¨
                container = link.find_parent(['div', 'article', 'section'])
                if not container:
                    container = link
                
                property_info = self.extract_property_info(container, object_id, href)
                if property_info:
                    properties.append(property_info)
                    print(f"âœ… è§£æç‰©ä»¶: {property_info.get('title', 'Unknown')[:30]}...")
                
            except Exception as e:
                print(f"âš ï¸  è§£æç‰©ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue
        
        return properties
    
    def extract_property_info(self, container, object_id: str, href: str) -> Optional[Dict[str, Any]]:
        """å¾å®¹å™¨ä¸­æå–æˆ¿å±‹è³‡è¨Š"""
        
        detail_url = urljoin(self.base_url, href)
        
        # å–å¾—å®¹å™¨å…§çš„æ‰€æœ‰æ–‡å­—
        container_text = container.get_text()
        
        # æå–æ¨™é¡Œ - é€šå¸¸æ˜¯é€£çµçš„æ–‡å­—
        title_elem = container.find('a', href=re.compile(r'/buy/house/'))
        raw_title = self.clean_text(title_elem.get_text()) if title_elem else "æœªçŸ¥ç‰©ä»¶"
        
        # å¦‚æœåŸå§‹æ¨™é¡Œå¤ªçŸ­ï¼Œå˜—è©¦å°‹æ‰¾æ›´è©³ç´°çš„æ¨™é¡Œ
        if len(raw_title) < 5:
            title_candidates = container.find_all(['h1', 'h2', 'h3', 'h4', 'h5'])
            for candidate in title_candidates:
                candidate_text = self.clean_text(candidate.get_text())
                if len(candidate_text) > len(raw_title):
                    raw_title = candidate_text
                    break
        
        # å¾å®Œæ•´æ¨™é¡Œä¸­æå–ç°¡æ½”çš„ç‰©ä»¶åç¨±
        title = self.extract_property_name(raw_title)
        
        # æå–åƒ¹æ ¼
        price = self.extract_price(container_text)
        
        # æå–åœ°å€
        address = self.extract_address(container_text)
        
        # æå–æˆ¿å‹è³‡è¨Š
        room_info = self.extract_room_info(container_text)
        
        # æå–åªæ•¸
        size_info = self.extract_size_info(container_text)
        
        # æå–æ¨“å±¤
        floor_info = self.extract_floor_info(container_text)
        
        # æå–å±‹é½¡
        age_info = self.extract_age_info(container_text)
        
        property_info = {
            'id': f"sinyi_sanchong_luzhou_{object_id}",
            'object_id': object_id,
            'title': title,
            'address': address,
            'district': self.district_name,
            'region': self.region_name,
            'price': price,
            'total_price': price,
            'room_count': room_info.get('rooms', 3),
            'living_room_count': room_info.get('living_rooms', 2),
            'bathroom_count': room_info.get('bathrooms', 2),
            'size': size_info.get('total_size', 0),
            'main_area': size_info.get('main_area', 0),
            'floor': floor_info,
            'age': age_info,
            'building_type': 'è¯å»ˆ/å¤§æ¨“',
            'source_site': 'ä¿¡ç¾©æˆ¿å±‹',
            'source_url': detail_url,
            'property_type': 'sale',
            'features': [],
            'equipment': [],
            'image_urls': [],
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        return property_info
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡å­—"""
        if not text:
            return ""
        return re.sub(r'\s+', ' ', str(text)).strip()
    
    def extract_property_name(self, title: str) -> str:
        """å¾å®Œæ•´æ¨™é¡Œä¸­æå–ç°¡æ½”çš„ç‰©ä»¶åç¨±"""
        if not title:
            return "æœªçŸ¥ç‰©ä»¶"
        
        # ç§»é™¤å¸¸è¦‹çš„å‰ç¶´
        title = re.sub(r'^(åº—é•·æ¨è–¦|å°ˆä»»|ç¨å®¶|æ€¥å”®|å‡ºåƒ¹å°±è«‡|å¯çœ‹|æ–°æ¥|ç¨€æœ‰|æ¨è–¦)\s*[â˜…â¤ï¸â­âœ¿ãŠ£\[\]ï½œÂ·]*\s*', '', title)
        
        # å˜—è©¦æå–ç¤¾å€åç¨±
        patterns = [
            # æ¨¡å¼1: ç¤¾å€åç¨±é‡è¤‡å‡ºç¾çš„æƒ…æ³ (å¦‚: "æ¦®è€€å·´é»...æ¦®è€€å·´é»æ–°åŒ—å¸‚")
            r'([A-Za-z\u4e00-\u9fff]{3,15}).*?\1æ–°åŒ—å¸‚',
            # æ¨¡å¼2: æ˜ç¢ºçš„ç¤¾å€åç¨± (å¦‚: "å…¨çƒå˜‰å¹´è¯æ–°åŒ—å¸‚")
            r'([A-Za-z\u4e00-\u9fff]{3,15})æ–°åŒ—å¸‚',
            # æ¨¡å¼3: ç¤¾å€åç¨±åœ¨æœ€æœ«å°¾ (å¦‚: "æ£®æ´»å¤§å¸‚æ–°åŒ—å¸‚") 
            r'([A-Za-z\u4e00-\u9fff]{3,12})æ–°åŒ—å¸‚[^A-Za-z\u4e00-\u9fff]*$',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, title)
            if match:
                extracted = match.group(1).strip()
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ç¤¾å€åç¨±
                if 3 <= len(extracted) <= 15:
                    # é¿å…ä¸€äº›é€šç”¨è©å½™
                    avoid_words = ['ä¸‰æˆ¿', 'å››æˆ¿', 'é›»æ¢¯', 'è»Šä½', 'å¤§æ¨“', 'è¯å»ˆ', 'å»ºåª', 'å¹´è¯', 'å¹´å¤§']
                    if not any(word in extracted for word in avoid_words):
                        return extracted
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç¤¾å€åç¨±ï¼Œå˜—è©¦æå–æè¿°æ€§æ¨™é¡Œ
        descriptive_patterns = [
            # æè¿° + ç¤¾å€åç¨±
            r'^([^æ–°åŒ—å°åŒ—0-9]{5,20}?)(?:æ–°åŒ—|å°åŒ—)',
            # ç´”æè¿°æ€§æ¨™é¡Œ
            r'^([^æ–°åŒ—å°åŒ—0-9]{3,15}?)(?:[0-9]+å¹´|å»ºåª)',
        ]
        
        for pattern in descriptive_patterns:
            match = re.search(pattern, title)
            if match:
                extracted = match.group(1).strip()
                # æ¸…ç†çµå°¾çš„å¸¸è¦‹è©å½™
                extracted = re.sub(r'(è»Šä½|ä¸‰æˆ¿|å››æˆ¿|é›»æ¢¯|æ™¯è§€|åº­é™¢|ç¶ æ„|é‚Šé–“|é«˜æ¨“|æ–¹æ­£|ç¾å­¸|è±ªé‚¸)$', '', extracted)
                if 3 <= len(extracted) <= 20:
                    return extracted
        
        # æœ€å¾Œçš„å‚™ç”¨æ–¹æ¡ˆï¼šå–å‰15å€‹å­—ç¬¦
        short_title = title[:15]
        for separator in ['æ–°åŒ—', 'å°åŒ—', 'å»ºåª', 'å¹´', 'æˆ¿', 'è¬']:
            if separator in short_title:
                short_title = short_title.split(separator)[0]
                break
        
        return short_title.strip() if short_title.strip() else "æœªçŸ¥ç‰©ä»¶"
    
    def extract_price(self, text: str) -> float:
        """æå–åƒ¹æ ¼ï¼ˆè¬å…ƒï¼‰"""
        # å°‹æ‰¾åƒ¹æ ¼æ¨¡å¼
        price_patterns = [
            r'(\d{1,4}(?:,\d{3})*(?:\.\d+)?)\s*è¬',
            r'ç¸½åƒ¹[ï¼š:\s]*(\d{1,4}(?:,\d{3})*(?:\.\d+)?)',
            r'(\d{3,4})\s*è¬'
        ]
        
        for pattern in price_patterns:
            price_match = re.search(pattern, text)
            if price_match:
                price_str = price_match.group(1).replace(',', '')
                try:
                    return float(price_str)
                except:
                    continue
        
        return 0
    
    def extract_address(self, text: str) -> str:
        """æå–åœ°å€"""
        # é‡å°ä¸‰é‡è˜†æ´²çš„åœ°å€æå–
        address_patterns = [
            r'(æ–°åŒ—å¸‚(?:ä¸‰é‡|è˜†æ´²)å€[^ï¼Œ\n]{1,30})',
            r'((?:ä¸‰é‡|è˜†æ´²)å€[^ï¼Œ\n]{1,30})',
            r'æ–°åŒ—å¸‚\s*(?:ä¸‰é‡|è˜†æ´²)å€\s*([^ï¼Œ\n]{1,20})'
        ]
        
        for pattern in address_patterns:
            match = re.search(pattern, text)
            if match:
                return self.clean_text(match.group(1))
        
        # å¦‚æœæ²’æ‰¾åˆ°å…·é«”åœ°å€ï¼Œæª¢æŸ¥æ˜¯å¦è‡³å°‘åŒ…å«å€åŸŸåç¨±
        if 'ä¸‰é‡' in text:
            return "ä¸‰é‡å€"
        elif 'è˜†æ´²' in text:
            return "è˜†æ´²å€"
        else:
            return "ä¸‰é‡è˜†æ´²å€"
    
    def extract_room_info(self, text: str) -> Dict[str, int]:
        """æå–æˆ¿å‹è³‡è¨Š"""
        # å„ªå…ˆå°‹æ‰¾æ¨™æº–æ ¼å¼
        room_match = re.search(r'(\d+)æˆ¿(\d+)å»³(\d+)è¡›', text)
        if room_match:
            rooms = int(room_match.group(1))
            living_rooms = int(room_match.group(2))
            bathrooms = int(room_match.group(3))
            
            # åˆç†æ€§æª¢æŸ¥
            if 1 <= rooms <= 10 and 1 <= living_rooms <= 5 and 1 <= bathrooms <= 5:
                return {
                    'rooms': rooms,
                    'living_rooms': living_rooms,
                    'bathrooms': bathrooms
                }
        
        # å°‹æ‰¾å…¶ä»–æ ¼å¼
        room_match2 = re.search(r'(\d+)R(\d+)L(\d+)B', text)
        if room_match2:
            rooms = int(room_match2.group(1))
            living_rooms = int(room_match2.group(2))
            bathrooms = int(room_match2.group(3))
            
            if 1 <= rooms <= 10 and 1 <= living_rooms <= 5 and 1 <= bathrooms <= 5:
                return {
                    'rooms': rooms,
                    'living_rooms': living_rooms,
                    'bathrooms': bathrooms
                }
        
        # åªå°‹æ‰¾æˆ¿é–“æ•¸
        room_only_match = re.search(r'(\d+)\s*æˆ¿', text)
        if room_only_match:
            rooms = int(room_only_match.group(1))
            if 1 <= rooms <= 10:
                return {
                    'rooms': rooms,
                    'living_rooms': 2,  # é è¨­å€¼
                    'bathrooms': 2      # é è¨­å€¼
                }
        
        # é è¨­å€¼
        return {'rooms': 3, 'living_rooms': 2, 'bathrooms': 2}
    
    def extract_size_info(self, text: str) -> Dict[str, float]:
        """æå–åªæ•¸è³‡è¨Š"""
        # å»ºåª
        size_patterns = [
            r'å»ºåª[ï¼š:\s]*(\d+(?:\.\d+)?)',
            r'ç¸½åªæ•¸[ï¼š:\s]*(\d+(?:\.\d+)?)',
            r'(\d+(?:\.\d+)?)\s*åª'
        ]
        
        total_size = 0
        for pattern in size_patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    total_size = float(match.group(1))
                    break
                except:
                    continue
        
        # ä¸»å»ºç‰©
        main_match = re.search(r'ä¸»å»ºç‰©[ï¼š:\s]*(\d+(?:\.\d+)?)', text)
        main_area = float(main_match.group(1)) if main_match else total_size * 0.8
        
        return {
            'total_size': total_size,
            'main_area': main_area
        }
    
    def extract_floor_info(self, text: str) -> str:
        """æå–æ¨“å±¤è³‡è¨Š"""
        floor_patterns = [
            r'(\d+)æ¨“/(\d+)æ¨“',
            r'(\d+)F/(\d+)F',
            r'(\d+)æ¨“',
            r'(\d+)F'
        ]
        
        for pattern in floor_patterns:
            floor_match = re.search(pattern, text)
            if floor_match:
                if len(floor_match.groups()) >= 2 and floor_match.group(2):
                    return f"{floor_match.group(1)}æ¨“/{floor_match.group(2)}æ¨“"
                else:
                    return f"{floor_match.group(1)}æ¨“"
        
        return "æœªçŸ¥æ¨“å±¤"
    
    def extract_age_info(self, text: str) -> int:
        """æå–å±‹é½¡"""
        age_patterns = [
            r'å±‹é½¡[ï¼š:\s]*(\d+(?:\.\d+)?)\s*å¹´',
            r'(\d+(?:\.\d+)?)\s*å¹´å±‹',
            r'æ°‘åœ‹\s*(\d+)\s*å¹´'
        ]
        
        for pattern in age_patterns:
            age_match = re.search(pattern, text)
            if age_match:
                try:
                    age = float(age_match.group(1))
                    # å¦‚æœæ˜¯æ°‘åœ‹å¹´ä»½ï¼Œè½‰æ›ç‚ºå±‹é½¡
                    if pattern.startswith(r'æ°‘åœ‹') and age > 50:
                        current_year = datetime.now().year - 1911  # æ°‘åœ‹å¹´
                        age = current_year - age
                    return int(age)
                except:
                    continue
        
        return 0
    
    def crawl_all_pages(self, max_pages: int = None) -> List[Dict[str, Any]]:
        """çˆ¬å–æ‰€æœ‰é é¢çš„ç‰©ä»¶"""
        print(f"ğŸ” é–‹å§‹çˆ¬å–ä¿¡ç¾©æˆ¿å±‹ä¸‰é‡è˜†æ´²è¯å»ˆå¤§æ¨“ç‰©ä»¶...")
        
        # ç²å–ç¸½é æ•¸
        detected_total_pages = self.get_total_pages()
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§é æ•¸é™åˆ¶ï¼Œå‰‡ä½¿ç”¨è¼ƒå°å€¼
        if max_pages is not None:
            total_pages = min(detected_total_pages, max_pages)
            print(f"ğŸ“„ æª¢æ¸¬åˆ° {detected_total_pages} é ï¼Œä½†é™åˆ¶ç‚º {max_pages} é ï¼Œå°‡çˆ¬å– {total_pages} é ")
        else:
            total_pages = detected_total_pages
            print(f"ğŸ“„ å°‡çˆ¬å–æ‰€æœ‰ {total_pages} é ")
        
        all_properties = []
        
        # çˆ¬å–æ¯ä¸€é 
        for page in range(1, total_pages + 1):
            page_url = f"{self.search_base_url}/{page}"
            print(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page}/{total_pages} é ...")
            
            html = self.fetch_page(page_url, delay=2.0)  # é©ç•¶å»¶é²é¿å…è¢«å°
            
            if html:
                page_properties = self.parse_property_list(html)
                
                # å¦‚æœç•¶å‰é é¢æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç‰©ä»¶ï¼Œå¯èƒ½æ˜¯åˆ°äº†æœ€å¾Œä¸€é 
                if not page_properties:
                    print(f"âš ï¸  ç¬¬ {page} é æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç‰©ä»¶ï¼Œå¯èƒ½å·²åˆ°é”æœ€å¾Œä¸€é ")
                    break
                
                all_properties.extend(page_properties)
                print(f"âœ… ç¬¬ {page} é æ‰¾åˆ° {len(page_properties)} å€‹ç‰©ä»¶")
                
                # æª¢æŸ¥æ˜¯å¦æ‰¾åˆ°é‡è¤‡çš„ç‰©ä»¶IDï¼ˆè¡¨ç¤ºå¯èƒ½å¾ªç’°åˆ°å·²çˆ¬éçš„é é¢ï¼‰
                if page > 1:
                    current_ids = {prop['object_id'] for prop in page_properties}
                    previous_ids = {prop['object_id'] for prop in all_properties[:-len(page_properties)]}
                    duplicate_ids = current_ids.intersection(previous_ids)
                    
                    if duplicate_ids:
                        print(f"âš ï¸  ç¬¬ {page} é ç™¼ç¾é‡è¤‡ç‰©ä»¶ï¼Œå¯èƒ½å·²åˆ°é”å¯¦éš›æœ€å¾Œä¸€é ")
                        # ç§»é™¤é‡è¤‡çš„ç‰©ä»¶
                        all_properties = all_properties[:-len(page_properties)]
                        break
                        
            else:
                print(f"âŒ ç¬¬ {page} é çˆ¬å–å¤±æ•—")
                if page > 1:  # å¦‚æœä¸æ˜¯ç¬¬ä¸€é å°±å¤±æ•—ï¼Œå¯èƒ½æ˜¯åˆ°äº†æœ€å¾Œ
                    print(f"âš ï¸  å¯èƒ½å·²åˆ°é”æœ€å¾Œä¸€é ")
                    break
        
        # å»é™¤é‡è¤‡ç‰©ä»¶ï¼ˆä»¥é˜²è¬ä¸€ï¼‰
        unique_properties = []
        seen_ids = set()
        for prop in all_properties:
            if prop['object_id'] not in seen_ids:
                unique_properties.append(prop)
                seen_ids.add(prop['object_id'])
        
        if len(unique_properties) != len(all_properties):
            print(f"âš ï¸  ç§»é™¤äº† {len(all_properties) - len(unique_properties)} å€‹é‡è¤‡ç‰©ä»¶")
        
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼ç¸½å…±æ‰¾åˆ° {len(unique_properties)} å€‹å”¯ä¸€ç‰©ä»¶")
        return unique_properties
    
    def save_to_local_file(self, properties: List[Dict[str, Any]], filename_prefix: str = "sanchong_luzhou_houses") -> str:
        """å„²å­˜åˆ°æœ¬åœ°æª”æ¡ˆ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        json_filename = f"data/{filename_prefix}_{timestamp}.json"
        
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(properties, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“ å·²å„²å­˜åˆ°: {json_filename}")
        return json_filename
    
    def load_previous_data(self) -> List[Dict[str, Any]]:
        """è¼‰å…¥å‰ä¸€å¤©çš„è³‡æ–™ç”¨æ–¼æ¯”è¼ƒ"""
        # å„ªå…ˆæª¢æŸ¥ GitHub Actions ä¸‹è¼‰çš„å‰ä¸€å¤©è³‡æ–™
        previous_data_dirs = ["./previous_data", "data"]
        
        print(f"ğŸ” æ­£åœ¨æœå°‹å‰ä¸€å¤©çš„ä¸‰é‡è˜†æ´²è³‡æ–™...")
        print(f"  â€¢ æœå°‹ç›®éŒ„: {previous_data_dirs}")
        
        for data_dir in previous_data_dirs:
            if not os.path.exists(data_dir):
                print(f"  âŒ {data_dir} ç›®éŒ„ä¸å­˜åœ¨")
                continue
                
            print(f"  ğŸ” åœ¨ {data_dir} ç›®éŒ„ä¸­æœå°‹å‰ä¸€å¤©çš„è³‡æ–™...")
            
            # åˆ—å‡ºç›®éŒ„ä¸­çš„æ‰€æœ‰æª”æ¡ˆ
            try:
                files_in_dir = os.listdir(data_dir)
                print(f"     ğŸ“„ æ‰¾åˆ° {len(files_in_dir)} å€‹æª”æ¡ˆ: {files_in_dir}")
            except Exception as e:
                print(f"     âŒ ç„¡æ³•åˆ—å‡ºæª”æ¡ˆ: {e}")
                continue
            
            # å¦‚æœæ˜¯ previous_data ç›®éŒ„ï¼ˆGitHub Actions ä¸‹è¼‰çš„ï¼‰
            if data_dir == "./previous_data":
                filename_prefix = "sanchong_luzhou_houses"
                print(f"     ğŸ¯ æœå°‹æª”æ¡ˆå‰ç¶´: {filename_prefix}")
                
                for filename in files_in_dir:
                    if filename.startswith(filename_prefix) and filename.endswith('.json'):
                        filepath = os.path.join(data_dir, filename)
                        print(f"     âœ… æ‰¾åˆ°åŒ¹é…æª”æ¡ˆ: {filename}")
                        try:
                            with open(filepath, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                print(f"     ğŸ“‚ å¾ GitHub Actions artifacts è¼‰å…¥å‰ä¸€å¤©è³‡æ–™: {len(data)} å€‹ç‰©ä»¶")
                                return data
                        except Exception as e:
                            print(f"     âŒ è¼‰å…¥å‰ä¸€å¤©è³‡æ–™å¤±æ•—: {str(e)}")
            else:
                # åŸæœ¬çš„é‚è¼¯ï¼šå°‹æ‰¾æ˜¨å¤©æ—¥æœŸçš„æª”æ¡ˆ
                yesterday = datetime.now() - timedelta(days=1)
                yesterday_str = yesterday.strftime('%Y%m%d')
                filename_prefix = "sanchong_luzhou_houses"
                target_pattern = f"{filename_prefix}_{yesterday_str}"
                
                print(f"     ğŸ¯ æœå°‹æ˜¨å¤©æ—¥æœŸæª”æ¡ˆæ¨¡å¼: {target_pattern}*.json")
                
                for filename in files_in_dir:
                    if filename.startswith(target_pattern) and filename.endswith('.json'):
                        filepath = os.path.join(data_dir, filename)
                        print(f"     âœ… æ‰¾åˆ°æ˜¨å¤©çš„æª”æ¡ˆ: {filename}")
                        try:
                            with open(filepath, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                print(f"     ğŸ“‚ è¼‰å…¥æ˜¨å¤©çš„è³‡æ–™: {len(data)} å€‹ç‰©ä»¶")
                                return data
                        except Exception as e:
                            print(f"     âŒ è¼‰å…¥æ˜¨å¤©è³‡æ–™å¤±æ•—: {str(e)}")
        
        print("ğŸ“‚ æœªæ‰¾åˆ°å‰ä¸€å¤©çš„è³‡æ–™")
        return []
    
    def compare_with_previous(self, current_properties: List[Dict[str, Any]], previous_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """èˆ‡å‰ä¸€å¤©çš„è³‡æ–™æ¯”è¼ƒï¼Œè©³ç´°åˆ†é¡å·®ç•°ç‰©ä»¶"""
        if not previous_data:
            return {
                'has_previous_data': False,
                'new_properties': current_properties,
                'removed_properties': [],
                'price_changed_properties': [],
                'unchanged_properties': [],
                'total_new': len(current_properties),
                'total_removed': 0,
                'total_price_changed': 0,
                'current_count': len(current_properties),
                'previous_count': 0,
                'message': 'é¦–æ¬¡çˆ¬å–ï¼Œæ‰€æœ‰ç‰©ä»¶éƒ½æ˜¯æ–°çš„'
            }
        
        # å»ºç«‹æ˜¨å¤©çš„ç‰©ä»¶æ˜ å°„ (ä½¿ç”¨åœ°å€+æˆ¿å‹+åªæ•¸ä½œç‚º key)
        previous_map = {}
        for prop in previous_data:
            key = self._generate_property_key(prop)
            previous_map[key] = prop
        
        # å»ºç«‹ä»Šå¤©çš„ç‰©ä»¶æ˜ å°„
        current_map = {}
        for prop in current_properties:
            key = self._generate_property_key(prop)
            current_map[key] = prop
        
        # æ‰¾å‡ºæ–°å¢çš„ç‰©ä»¶
        new_properties = []
        unchanged_properties = []
        price_changed_properties = []
        
        for key, current_prop in current_map.items():
            if key not in previous_map:
                # æ–°å¢çš„ç‰©ä»¶
                new_properties.append(current_prop)
                print(f"ğŸ†• æ–°å¢ç‰©ä»¶: {current_prop.get('title', 'Unknown')[:30]}")
            else:
                # å­˜åœ¨çš„ç‰©ä»¶ï¼Œæª¢æŸ¥åƒ¹æ ¼æ˜¯å¦è®Šå‹•
                previous_prop = previous_map[key]
                current_price = current_prop.get('price', 0)
                previous_price = previous_prop.get('price', 0)
                
                if abs(current_price - previous_price) > 0:  # åƒ¹æ ¼æœ‰è®Šå‹•
                    change_info = {
                        'property': current_prop,
                        'old_price': previous_price,
                        'new_price': current_price,
                        'change': current_price - previous_price,
                        'change_percentage': ((current_price - previous_price) / previous_price * 100) if previous_price > 0 else 0
                    }
                    price_changed_properties.append(change_info)
                    
                    change_emoji = "ğŸ“ˆ" if change_info['change'] > 0 else "ğŸ“‰"
                    print(f"{change_emoji} è®Šåƒ¹ç‰©ä»¶: {current_prop.get('title', 'Unknown')[:30]} - {previous_price:,}â†’{current_price:,}è¬ ({change_info['change']:+.0f}è¬)")
                else:
                    # åƒ¹æ ¼ç„¡è®Šå‹•çš„ç‰©ä»¶
                    unchanged_properties.append(current_prop)
        
        # æ‰¾å‡ºä¸‹æ¶çš„ç‰©ä»¶
        removed_properties = []
        for key, previous_prop in previous_map.items():
            if key not in current_map:
                removed_properties.append(previous_prop)
                print(f"ğŸ“¤ ä¸‹æ¶ç‰©ä»¶: {previous_prop.get('title', 'Unknown')[:30]}")
        
        # è¨ˆç®—è®ŠåŒ–
        change = len(current_properties) - len(previous_data)
        
        # ç”Ÿæˆè©³ç´°çš„æ¯”è¼ƒæ‘˜è¦
        summary_parts = []
        if new_properties:
            summary_parts.append(f"æ–°å¢ {len(new_properties)} å€‹")
        if removed_properties:
            summary_parts.append(f"ä¸‹æ¶ {len(removed_properties)} å€‹")
        if price_changed_properties:
            summary_parts.append(f"è®Šåƒ¹ {len(price_changed_properties)} å€‹")
        
        if not summary_parts:
            summary_parts.append("ç„¡è®ŠåŒ–")
        
        message = f"èˆ‡æ˜¨å¤©æ¯”è¼ƒï¼š{', '.join(summary_parts)}ç‰©ä»¶"
        
        return {
            'has_previous_data': True,
            'new_properties': new_properties,
            'removed_properties': removed_properties,
            'price_changed_properties': price_changed_properties,
            'unchanged_properties': unchanged_properties,
            'total_new': len(new_properties),
            'total_removed': len(removed_properties),
            'total_price_changed': len(price_changed_properties),
            'current_count': len(current_properties),
            'previous_count': len(previous_data),
            'change': change,
            'message': message
        }
    
    def _generate_property_key(self, prop: Dict[str, Any]) -> str:
        """ç”Ÿæˆç‰©ä»¶çš„å”¯ä¸€è­˜åˆ¥éµ"""
        address = prop.get('address', '').strip()
        room_count = prop.get('room_count', 0)
        size = prop.get('size', 0) or prop.get('main_area', 0)
        
        # ä½¿ç”¨åœ°å€ã€æˆ¿æ•¸ã€åªæ•¸ä½œç‚ºå”¯ä¸€è­˜åˆ¥
        return f"{address}_{room_count}_{size}"
    
    def upload_to_notion(self, properties: List[Dict[str, Any]], comparison_data: Dict = None) -> bool:
        """ä¸Šå‚³åˆ° Notion"""
        notion_token = os.getenv('NOTION_API_TOKEN', os.getenv('NOTION_TOKEN'))
        
        if not notion_token:
            print("âš ï¸  æœªè¨­å®š NOTION_API_TOKENï¼Œè·³é Notion ä¸Šå‚³")
            print("ğŸ’¡ è«‹è¨­å®šç’°å¢ƒè®Šæ•¸æˆ–åŸ·è¡Œ setup_notion.py")
            return False
        
        # èª¿è©¦ï¼šé¡¯ç¤ºæ¯”è¼ƒè³‡æ–™çš„è©³ç´°è³‡è¨Š
        print(f"\nğŸ” æ¯”è¼ƒè³‡æ–™èª¿è©¦è³‡è¨Š:")
        if comparison_data:
            print(f"  â€¢ has_previous_data: {comparison_data.get('has_previous_data', False)}")
            print(f"  â€¢ æ–°å¢ç‰©ä»¶æ•¸é‡: {len(comparison_data.get('new_properties', []))}")
            print(f"  â€¢ åƒ¹æ ¼è®Šå‹•ç‰©ä»¶æ•¸é‡: {len(comparison_data.get('price_changed_properties', []))}")
            print(f"  â€¢ ä¸‹æ¶ç‰©ä»¶æ•¸é‡: {len(comparison_data.get('removed_properties', []))}")
            print(f"  â€¢ ç¸½è¨ˆç›®å‰ç‰©ä»¶: {comparison_data.get('current_count', 0)}")
            print(f"  â€¢ ç¸½è¨ˆå‰ä¸€å¤©ç‰©ä»¶: {comparison_data.get('previous_count', 0)}")
        else:
            print(f"  â€¢ comparison_data ç‚º None")
        
        try:
            # å˜—è©¦åŒ¯å…¥ Notion åŠŸèƒ½
            from src.utils.full_notion import create_full_notion_client
            from src.models.property import Property
            
            print("ğŸ”„ æ­£åœ¨ä¸Šå‚³åˆ° Notion...")
            
            client = create_full_notion_client(notion_token)
            
            if not client.test_connection():
                print("âŒ Notion API é€£æ¥å¤±æ•—")
                return False
            
            # å°‡å­—å…¸è³‡æ–™è½‰æ›ç‚º Property å°è±¡
            property_objects = []
            for i, prop_dict in enumerate(properties):
                try:
                    # ç”Ÿæˆå”¯ä¸€ ID
                    prop_id = f"sanchong_luzhou_{i+1}_{prop_dict.get('title', '')[:10]}"
                    
                    prop = Property(
                        id=prop_id,
                        title=prop_dict.get('title', ''),
                        address=prop_dict.get('address', ''),
                        district=self.district_name,
                        region=self.region_name,
                        price=int(prop_dict.get('price', 0)),
                        room_count=prop_dict.get('room_count', 3),
                        living_room_count=prop_dict.get('living_room_count', 2),
                        bathroom_count=prop_dict.get('bathroom_count', 2),
                        size=prop_dict.get('size', 0),
                        floor=str(prop_dict.get('floor', '')),
                        source_site="ä¿¡ç¾©æˆ¿å±‹",
                        source_url=prop_dict.get('source_url', ''),
                        property_type='sale'  # è²·å±‹
                    )
                    
                    # è¨­å®šè²·å±‹å°ˆç”¨æ¬„ä½
                    prop.total_price = int(prop_dict.get('price', 0))  # ç¸½åƒ¹
                    
                    # è¨­å®šå…¶ä»–å±¬æ€§ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                    if 'main_area' in prop_dict:
                        prop.main_area = prop_dict['main_area']
                    if 'unit_price' in prop_dict:
                        prop.unit_price = prop_dict['unit_price']
                    if 'total_floors' in prop_dict:
                        prop.total_floors = prop_dict['total_floors']
                    if 'age' in prop_dict:
                        prop.age = prop_dict['age']
                    if 'building_type' in prop_dict:
                        prop.building_type = prop_dict['building_type']
                    
                    property_objects.append(prop)
                except Exception as e:
                    print(f"âš ï¸  è½‰æ›ç‰©ä»¶å¤±æ•—: {e}")
                    continue
            
            # ä½¿ç”¨æ–°çš„å±¤ç´šçµæ§‹ä¸Šå‚³
            today = datetime.now()
            district_name = 'sanchong_luzhou'  # è‹±æ–‡å€åŸŸåç¨±
            
            # ä½¿ç”¨æ–°çš„å€åŸŸç‰©ä»¶æ¸…å–®å‰µå»ºæ–¹æ³•ï¼Œå‚³å…¥æ¯”è¼ƒè³‡æ–™
            success = client.create_district_house_list(
                properties=property_objects,
                search_date=today,
                district_name=district_name,
                comparison=comparison_data  # å‚³å…¥æ¯”è¼ƒè³‡æ–™
            )
            
            if success:
                print("âœ… æˆåŠŸä¸Šå‚³åˆ° Notionï¼")
                return True
            else:
                print("âŒ Notion ä¸Šå‚³å¤±æ•—")
                return False
                
        except ImportError:
            print("âš ï¸  ç„¡æ³•è¼‰å…¥ Notion åŠŸèƒ½ï¼Œè·³éä¸Šå‚³")
            return False
        except Exception as e:
            print(f"âŒ Notion ä¸Šå‚³éŒ¯èª¤: {str(e)}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸ  ä¿¡ç¾©æˆ¿å±‹ä¸‰é‡è˜†æ´²è¯å»ˆå¤§æ¨“æ•´åˆçˆ¬èŸ²")
    print("ğŸ”— æœå°‹ç¶²å€: https://www.sinyi.com.tw/buy/list/3000-down-price/dalou-huaxia-type/20-up-balconyarea/3-5-roomtotal/NewTaipei-city/241-247-zip/default-desc")
    print("=" * 80)
    
    try:
        crawler = SanchongLuzhouCrawler()
        
        # 1. è¼‰å…¥å‰ä¸€å¤©çš„è³‡æ–™
        print("ğŸ“‚ è¼‰å…¥å‰ä¸€å¤©çš„è³‡æ–™...")
        previous_data = crawler.load_previous_data()
        
        # 2. çˆ¬å–ä»Šå¤©çš„è³‡æ–™
        properties = crawler.crawl_all_pages()
        
        if not properties:
            print("âŒ æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•è³‡æ–™")
            return
        
        # 3. èˆ‡å‰ä¸€å¤©æ¯”è¼ƒ
        comparison = crawler.compare_with_previous(properties, previous_data)
        print(f"\nğŸ“ˆ {comparison['message']}")
        
        # 4. é¡¯ç¤ºè©³ç´°æ¯”è¼ƒçµæœ
        print(f"\nğŸ“Š ä¸‰é‡è˜†æ´²å€æ¯”è¼ƒè©³æƒ…:")
        print(f"  â€¢ å‰ä¸€å¤©ç‰©ä»¶æ•¸: {comparison.get('previous_count', 0)}")
        print(f"  â€¢ ä»Šå¤©ç‰©ä»¶æ•¸: {comparison.get('current_count', 0)}")
        print(f"  â€¢ ğŸ†• æ–°å¢ç‰©ä»¶æ•¸: {comparison.get('total_new', 0)}")
        print(f"  â€¢ ğŸ“¤ ä¸‹æ¶ç‰©ä»¶æ•¸: {comparison.get('total_removed', 0)}")
        print(f"  â€¢ ğŸ’° è®Šåƒ¹ç‰©ä»¶æ•¸: {comparison.get('total_price_changed', 0)}")
        
        # 5. é¡¯ç¤ºé‡é»ç‰©ä»¶é è¦½
        if comparison.get('new_properties'):
            print(f"\nğŸ’¡ ğŸ†• æ–°å¢ç‰©ä»¶é è¦½ (å…± {len(comparison['new_properties'])} å€‹):")
            for i, prop in enumerate(comparison['new_properties'][:3], 1):
                print(f"  {i}. {prop['title'][:30]}...")
                print(f"     ğŸ’° åƒ¹æ ¼: {prop['price']:,} è¬å…ƒ")
                print(f"     ğŸ“ åœ°å€: {prop['address']}")
                print(f"     ğŸ¢ æˆ¿å‹: {prop['room_count']}æˆ¿{prop['living_room_count']}å»³{prop['bathroom_count']}è¡›")
                print(f"     ğŸ“ åªæ•¸: {prop['size']} åª")
                print(f"     ğŸ—ï¸ æ¨“å±¤: {prop['floor']}")
                print(f"     ğŸ”— æŸ¥çœ‹è©³æƒ…: {prop['source_url']}")
                print()
            
            if len(comparison['new_properties']) > 3:
                print(f"     ... åŠå…¶ä»– {len(comparison['new_properties']) - 3} å€‹æ–°å¢ç‰©ä»¶")
        
        if comparison.get('price_changed_properties'):
            print(f"\nğŸ’° è®Šåƒ¹ç‰©ä»¶é è¦½ (å…± {len(comparison['price_changed_properties'])} å€‹):")
            for i, change_info in enumerate(comparison['price_changed_properties'][:2], 1):
                prop = change_info['property']
                old_price = change_info['old_price']
                new_price = change_info['new_price']
                change_amount = change_info['change']
                
                change_symbol = "ğŸ“ˆ" if change_amount > 0 else "ğŸ“‰"
                change_text = f"+{change_amount}" if change_amount > 0 else str(change_amount)
                
                print(f"  {i}. {prop['title'][:30]}...")
                print(f"     ğŸ’° åƒ¹æ ¼ç•°å‹•: {old_price:,} â†’ {new_price:,} è¬å…ƒ ({change_symbol} {change_text} è¬)")
                print(f"     ğŸ“ åœ°å€: {prop['address']}")
                print(f"     ğŸ¢ æˆ¿å‹: {prop['room_count']}æˆ¿{prop['living_room_count']}å»³{prop['bathroom_count']}è¡›")
                print(f"     ğŸ“ åªæ•¸: {prop['size']} åª")
                print(f"     ğŸ—ï¸ æ¨“å±¤: {prop['floor']}")
                print(f"     ğŸ”— æŸ¥çœ‹è©³æƒ…: {prop['source_url']}")
                print()
            
            if len(comparison['price_changed_properties']) > 2:
                print(f"     ... åŠå…¶ä»– {len(comparison['price_changed_properties']) - 2} å€‹è®Šåƒ¹ç‰©ä»¶")
        
        if comparison.get('removed_properties'):
            print(f"\nğŸ“¤ ä¸‹æ¶ç‰©ä»¶æ•¸é‡: {len(comparison['removed_properties'])} å€‹")
        
        # 6. å„²å­˜æœ¬åœ°æª”æ¡ˆ
        json_file = crawler.save_to_local_file(properties)
        
        # 7. å˜—è©¦ä¸Šå‚³åˆ° Notion
        success = crawler.upload_to_notion(properties, comparison)
        
        # 8. ç¸½çµ
        print(f"\nğŸ“Š ä¸‰é‡è˜†æ´²å€çˆ¬å–ç¸½çµ:")
        print(f"  â€¢ ä»Šå¤©æ‰¾åˆ°ç‰©ä»¶: {len(properties)} å€‹")
        
        if comparison['has_previous_data']:
            print(f"  â€¢ æ˜¨å¤©ç‰©ä»¶æ•¸é‡: {comparison['previous_count']} å€‹")
            print(f"  â€¢ ğŸ†• æ–°å¢ç‰©ä»¶: {comparison['total_new']} å€‹")
            print(f"  â€¢ ğŸ“¤ ä¸‹æ¶ç‰©ä»¶: {comparison['total_removed']} å€‹")
            print(f"  â€¢ ğŸ’° è®Šåƒ¹ç‰©ä»¶: {comparison['total_price_changed']} å€‹")
            
            # é¡¯ç¤ºæ·¨è®ŠåŒ–
            net_change = comparison.get('change', 0)
            if net_change > 0:
                print(f"  â€¢ ğŸ“ˆ æ·¨å¢åŠ : +{net_change} å€‹")
            elif net_change < 0:
                print(f"  â€¢ ğŸ“‰ æ·¨æ¸›å°‘: {abs(net_change)} å€‹")
            else:
                print(f"  â€¢ â¡ï¸ æ•¸é‡ç„¡è®ŠåŒ–")
        else:
            print(f"  â€¢ ğŸ†• æ–°å¢ç‰©ä»¶: {comparison['total_new']} å€‹ (é¦–æ¬¡åŸ·è¡Œ)")
        
        print(f"  â€¢ ğŸ“ æœ¬åœ°æª”æ¡ˆ: {json_file}")
        print(f"  â€¢ ğŸ”— Notionä¸Šå‚³: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'}")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        print(f"\nâŒ åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
