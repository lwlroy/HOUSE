#!/usr/bin/env python3
"""
ä¿¡ç¾©æˆ¿å±‹å°åŒ—å…¬å¯“ç°¡åŒ–ç‰ˆçˆ¬èŸ²
åªæ”¯æ´å°åŒ—å€åŸŸçš„å…¬å¯“ç‰©ä»¶çˆ¬å–
"""

import json
import re
import os
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin
import sys
from pathlib import Path

# å˜—è©¦åŒ¯å…¥å¥—ä»¶
try:
    import requests
    from bs4 import BeautifulSoup
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦å¥—ä»¶: {e}")
    print("è«‹å®‰è£: pip3 install requests beautifulsoup4 --user")
    sys.exit(1)

# åŠ å…¥å°ˆæ¡ˆè·¯å¾‘
sys.path.append(str(Path(__file__).parent))

try:
    from src.models.property import Property
    from src.utils.full_notion import create_full_notion_client
except ImportError as e:
    print(f"âš ï¸  ç„¡æ³•è¼‰å…¥å°ˆæ¡ˆæ¨¡çµ„: {e}")
    print("å°‡ä½¿ç”¨ç°¡åŒ–æ¨¡å¼é‹è¡Œ...")
    Property = None


class TaipeiApartmentCrawler:
    """ä¿¡ç¾©æˆ¿å±‹å°åŒ—å…¬å¯“çˆ¬èŸ²ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.base_url = "https://www.sinyi.com.tw"
        self.search_url = "https://www.sinyi.com.tw/buy/list/3000-down-price/apartment-type/20-up-balconyarea/3-5-roomtotal/1-3-floor/Taipei-city/100-103-104-105-106-108-110-115-zip/default-desc"
        self.district_name = "å°åŒ—"
        self.region_name = "å°åŒ—å¸‚"
        
        print(f"ğŸ¯ è¨­å®šçˆ¬èŸ²å€åŸŸ: {self.district_name}å€å…¬å¯“")
        print(f"ğŸ”— æœå°‹ç¶²å€: {self.search_url}")
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        # è™•ç† SSL æ†‘è­‰å•é¡Œ
        self.session.verify = False
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs("data", exist_ok=True)
    
    def get_total_pages(self) -> int:
        """ç¢ºå®šç¸½é æ•¸"""
        print(f"ğŸ“„ å˜—è©¦é€šéæª¢æŸ¥é é¢å­˜åœ¨æ€§ä¾†ç¢ºå®šç¸½é æ•¸...")
        
        for page in range(1, 20):  # æœ€å¤šæª¢æŸ¥20é 
            print(f"   æª¢æŸ¥ç¬¬ {page} é ...")
            
            page_url = f"{self.search_url}/{page}"
            try:
                response = self.session.get(page_url, timeout=10)
                print(f"âœ… æˆåŠŸç²å–é é¢ï¼Œå…§å®¹é•·åº¦: {len(response.content)}")
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ç‰©ä»¶
                items = soup.find_all('div', class_='buy-list-item')
                
                if len(items) == 0:
                    print(f"   âŒ ç¬¬ {page} é ç„¡è³‡æ–™ï¼Œåœæ­¢æª¢æŸ¥")
                    return page - 1
                else:
                    print(f"   âœ… ç¬¬ {page} é æœ‰è³‡æ–™")
                
                time.sleep(2)  # é¿å…è«‹æ±‚éå¿«
                
            except Exception as e:
                print(f"   âŒ ç¬¬ {page} é æª¢æŸ¥å¤±æ•—: {str(e)}")
                return page - 1
        
        return 15  # é»˜èªæœ€å¤§é æ•¸
    
    def crawl_page(self, page: int = 1) -> List[Dict[str, Any]]:
        """çˆ¬å–æŒ‡å®šé é¢çš„ç‰©ä»¶"""
        properties = []
        
        page_url = f"{self.search_url}/{page}"
        print(f"ğŸ” æ­£åœ¨ç²å–: {page_url}")
        
        try:
            response = self.session.get(page_url, timeout=15)
            print(f"âœ… æˆåŠŸç²å–é é¢ï¼Œå…§å®¹é•·åº¦: {len(response.content)}")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # å°‹æ‰¾ç‰©ä»¶é€£çµ
            items = soup.find_all('div', class_='buy-list-item')
            
            property_links = []
            for item in items:
                link_element = item.find('a', href=True)
                if link_element:
                    href = link_element['href']
                    if href.startswith('/buy/house/'):
                        full_url = urljoin(self.base_url, href)
                        property_links.append(full_url)
            
            print(f"ğŸ  æ‰¾åˆ° {len(property_links)} å€‹ç‰©ä»¶é€£çµ")
            
            # çˆ¬å–æ¯å€‹ç‰©ä»¶çš„è©³ç´°è³‡è¨Š
            for i, link in enumerate(property_links, 1):
                try:
                    prop = self.parse_property_detail(link)
                    if prop:
                        properties.append(prop)
                        print(f"âœ… è§£æç‰©ä»¶: {prop['title'][:20]}...")
                    
                    # é¿å…è«‹æ±‚éå¿«
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"âŒ è§£æç‰©ä»¶å¤±æ•—: {str(e)}")
                    continue
            
        except Exception as e:
            print(f"âŒ çˆ¬å–ç¬¬ {page} é å¤±æ•—: {str(e)}")
        
        return properties
    
    def parse_property_detail(self, url: str) -> Optional[Dict[str, Any]]:
        """è§£æç‰©ä»¶è©³ç´°è³‡è¨Š"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–åŸºæœ¬è³‡è¨Š
            title = self._extract_title(soup)
            price = self._extract_price(soup)
            address = self._extract_address(soup)
            room_info = self._extract_room_info(soup)
            size_info = self._extract_size_info(soup)
            floor_info = self._extract_floor_info(soup)
            
            # ç”Ÿæˆç‰©ä»¶ID
            object_id = url.split('/')[-1].split('?')[0] if '/' in url else 'unknown'
            
            property_data = {
                'id': f"taipei_{object_id}",
                'object_id': object_id,
                'title': title,
                'address': address,
                'price': price,
                'room_count': room_info.get('room_count', 3),
                'living_room_count': room_info.get('living_room_count', 2),
                'bathroom_count': room_info.get('bathroom_count', 2),
                'size': size_info.get('total_size', 0),
                'main_area': size_info.get('main_area', size_info.get('total_size', 0)),
                'floor': floor_info,
                'source_url': url,
                'crawl_time': datetime.now().isoformat(),
                'region': self.region_name,
                'district': self.district_name
            }
            
            return property_data
            
        except Exception as e:
            print(f"âŒ è§£æç‰©ä»¶è©³æƒ…å¤±æ•—: {str(e)}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """æå–æ¨™é¡Œ"""
        title_selectors = [
            'h1.object-title',
            'h1',
            '.object-title',
            '.title'
        ]
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text(strip=True)
        
        return "æœªçŸ¥ç‰©ä»¶"
    
    def _extract_price(self, soup: BeautifulSoup) -> int:
        """æå–åƒ¹æ ¼ï¼ˆè¬å…ƒï¼‰"""
        price_selectors = [
            '.object-price .price-total',
            '.price-total',
            '.object-price'
        ]
        
        for selector in price_selectors:
            element = soup.select_one(selector)
            if element:
                price_text = element.get_text(strip=True)
                # æå–æ•¸å­—
                price_match = re.search(r'[\d,]+', price_text.replace(',', ''))
                if price_match:
                    try:
                        return int(price_match.group())
                    except ValueError:
                        continue
        
        return 0
    
    def _extract_address(self, soup: BeautifulSoup) -> str:
        """æå–åœ°å€"""
        address_selectors = [
            '.object-address',
            '.address',
            '.location'
        ]
        
        for selector in address_selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text(strip=True)
        
        return "æœªçŸ¥åœ°å€"
    
    def _extract_room_info(self, soup: BeautifulSoup) -> Dict[str, int]:
        """æå–æˆ¿é–“è³‡è¨Š"""
        room_info = {'room_count': 3, 'living_room_count': 2, 'bathroom_count': 2}
        
        # å°‹æ‰¾æˆ¿å‹è³‡è¨Š
        room_elements = soup.find_all(text=re.compile(r'\d+æˆ¿\d+å»³\d+è¡›'))
        for element in room_elements:
            room_match = re.search(r'(\d+)æˆ¿(\d+)å»³(\d+)è¡›', element)
            if room_match:
                room_info['room_count'] = int(room_match.group(1))
                room_info['living_room_count'] = int(room_match.group(2))
                room_info['bathroom_count'] = int(room_match.group(3))
                break
        
        return room_info
    
    def _extract_size_info(self, soup: BeautifulSoup) -> Dict[str, float]:
        """æå–åªæ•¸è³‡è¨Š"""
        size_info = {'total_size': 0, 'main_area': 0}
        
        # å°‹æ‰¾åªæ•¸è³‡è¨Š
        size_elements = soup.find_all(text=re.compile(r'\d+\.?\d*åª'))
        for element in size_elements:
            size_match = re.search(r'(\d+\.?\d*)åª', element)
            if size_match:
                size_value = float(size_match.group(1))
                if size_value > size_info['total_size']:
                    size_info['total_size'] = size_value
                    size_info['main_area'] = size_value
        
        return size_info
    
    def _extract_floor_info(self, soup: BeautifulSoup) -> str:
        """æå–æ¨“å±¤è³‡è¨Š"""
        floor_patterns = [
            r'(\d+)æ¨“/(\d+)æ¨“',
            r'(\d+)F/(\d+)F',
            r'(\d+)æ¨“',
            r'(\d+)F'
        ]
        
        # åœ¨æ‰€æœ‰æ–‡æœ¬ä¸­æœå°‹æ¨“å±¤è³‡è¨Š
        all_text = soup.get_text()
        for pattern in floor_patterns:
            floor_match = re.search(pattern, all_text)
            if floor_match:
                if len(floor_match.groups()) >= 2 and floor_match.group(2):
                    return f"{floor_match.group(1)}æ¨“/{floor_match.group(2)}æ¨“"
                else:
                    return f"{floor_match.group(1)}æ¨“"
        
        return "æœªçŸ¥æ¨“å±¤"
    
    def crawl_all_pages(self) -> List[Dict[str, Any]]:
        """çˆ¬å–æ‰€æœ‰é é¢"""
        print("ğŸ” é–‹å§‹çˆ¬å–ä¿¡ç¾©æˆ¿å±‹å°åŒ—å…¬å¯“ç‰©ä»¶...")
        
        total_pages = self.get_total_pages()
        print(f"ğŸ“„ ç¢ºå®šç¸½é æ•¸: {total_pages}")
        print(f"ğŸ“„ å°‡çˆ¬å–æ‰€æœ‰ {total_pages} é ")
        
        all_properties = []
        
        for page in range(1, total_pages + 1):
            print(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page}/{total_pages} é ...")
            
            try:
                properties = self.crawl_page(page)
                all_properties.extend(properties)
                print(f"âœ… ç¬¬ {page} é æ‰¾åˆ° {len(properties)} å€‹ç‰©ä»¶")
                
                # é¿å…è«‹æ±‚éå¿«
                time.sleep(3)
                
            except Exception as e:
                print(f"âŒ ç¬¬ {page} é çˆ¬å–å¤±æ•—: {str(e)}")
                continue
        
        # å»é‡
        unique_properties = []
        seen_ids = set()
        
        for prop in all_properties:
            prop_id = prop.get('object_id', '')
            if prop_id and prop_id not in seen_ids:
                seen_ids.add(prop_id)
                unique_properties.append(prop)
        
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼ç¸½å…±æ‰¾åˆ° {len(unique_properties)} å€‹å”¯ä¸€ç‰©ä»¶")
        
        return unique_properties
    
    def save_to_local_file(self, properties: List[Dict[str, Any]]) -> str:
        """å„²å­˜åˆ°æœ¬åœ°JSONæª”æ¡ˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"data/taipei_houses_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(properties, f, ensure_ascii=False, indent=2)
        
        return filename
    
    def load_previous_data(self) -> List[Dict[str, Any]]:
        """è¼‰å…¥å‰ä¸€å¤©çš„è³‡æ–™"""
        print("ğŸ” æ­£åœ¨æœå°‹å‰ä¸€å¤©çš„è³‡æ–™...")
        print(f"  â€¢ ç›®æ¨™å€åŸŸ: taipei")
        
        data_dirs = ["./previous_data", "data"]
        print(f"  â€¢ æœå°‹ç›®éŒ„: {data_dirs}")
        
        for data_dir in data_dirs:
            if not os.path.exists(data_dir):
                print(f"  âŒ {data_dir} ç›®éŒ„ä¸å­˜åœ¨")
                continue
            
            print(f"  ğŸ” åœ¨ {data_dir} ç›®éŒ„ä¸­æœå°‹å‰ä¸€å¤©çš„è³‡æ–™...")
            
            try:
                files = os.listdir(data_dir)
                print(f"     ğŸ“„ æ‰¾åˆ° {len(files)} å€‹æª”æ¡ˆ: {files}")
                
                # æœå°‹æ˜¨å¤©çš„æª”æ¡ˆ
                yesterday = datetime.now() - timedelta(days=1)
                yesterday_pattern = f"taipei_houses_{yesterday.strftime('%Y%m%d')}"
                print(f"     ğŸ¯ æœå°‹æ˜¨å¤©æ—¥æœŸæª”æ¡ˆæ¨¡å¼: {yesterday_pattern}*.json")
                
                matching_files = [f for f in files if f.startswith(yesterday_pattern) and f.endswith('.json')]
                
                if matching_files:
                    # å–æœ€æ–°çš„æª”æ¡ˆ
                    latest_file = sorted(matching_files)[-1]
                    filepath = os.path.join(data_dir, latest_file)
                    
                    print(f"  âœ… æ‰¾åˆ°å‰ä¸€å¤©è³‡æ–™: {latest_file}")
                    
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        print(f"  ğŸ“Š è¼‰å…¥ {len(data)} å€‹å‰ä¸€å¤©ç‰©ä»¶")
                        return data
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°æ˜¨å¤©çš„æª”æ¡ˆï¼Œå°‹æ‰¾æœ€æ–°çš„å°åŒ—æª”æ¡ˆ
                    taipei_files = [f for f in files if f.startswith("taipei_houses_") and f.endswith('.json')]
                    if taipei_files:
                        # æŒ‰æª”åæ’åºï¼Œå–æœ€æ–°çš„
                        taipei_files.sort(reverse=True)
                        latest_file = taipei_files[0]
                        filepath = os.path.join(data_dir, latest_file)
                        print(f"  âœ… æ‰¾åˆ°æœ€æ–°çš„å°åŒ—æª”æ¡ˆ: {latest_file}")
                        
                        with open(filepath, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            print(f"  ğŸ“Š è¼‰å…¥æœ€æ–°è³‡æ–™: {len(data)} å€‹ç‰©ä»¶")
                            return data
                
            except Exception as e:
                print(f"  âŒ è®€å– {data_dir} å¤±æ•—: {str(e)}")
                continue
        
        print("ğŸ“‚ æœªæ‰¾åˆ°å‰ä¸€å¤©çš„è³‡æ–™")
        return []
    
    def compare_with_previous(self, current_properties: List[Dict[str, Any]], previous_data: List[Dict[str, Any]]) -> Dict:
        """èˆ‡å‰ä¸€å¤©è³‡æ–™æ¯”è¼ƒ"""
        if not previous_data:
            return {
                'has_previous_data': False,
                'new_properties': current_properties,
                'removed_properties': [],
                'price_changed_properties': [],
                'unchanged_properties': [],
                'total_new': len(current_properties),
                'total_removed': 0,
                'total_price_changed': 0,
                'current_count': len(current_properties),
                'previous_count': 0,
                'change': len(current_properties),
                'message': 'é¦–æ¬¡çˆ¬å–ï¼Œæ‰€æœ‰ç‰©ä»¶éƒ½æ˜¯æ–°çš„'
            }
        
        # ä½¿ç”¨åœ°å€ã€æˆ¿æ•¸ã€åªæ•¸ä½œç‚ºå”¯ä¸€è­˜åˆ¥
        def generate_key(prop):
            address = prop.get('address', '').strip()
            room_count = prop.get('room_count', 0)
            size = prop.get('size', 0)
            main_area = prop.get('main_area', size)
            return f"{address}_{room_count}_{main_area}"
        
        # å»ºç«‹å‰ä¸€å¤©çš„ç‰©ä»¶æ˜ å°„
        previous_map = {}
        for prop in previous_data:
            key = generate_key(prop)
            previous_map[key] = prop
        
        # å»ºç«‹ä»Šå¤©çš„ç‰©ä»¶æ˜ å°„
        current_map = {}
        for prop in current_properties:
            key = generate_key(prop)
            current_map[key] = prop
        
        # åˆ†æè®ŠåŒ–
        new_properties = []
        unchanged_properties = []
        price_changed_properties = []
        
        for key, current_prop in current_map.items():
            if key not in previous_map:
                new_properties.append(current_prop)
            else:
                previous_prop = previous_map[key]
                current_price = current_prop.get('price', 0)
                previous_price = previous_prop.get('price', 0)
                
                if abs(current_price - previous_price) > 0:
                    price_changed_properties.append({
                        'property': current_prop,
                        'old_price': previous_price,
                        'new_price': current_price,
                        'change': current_price - previous_price
                    })
                else:
                    unchanged_properties.append(current_prop)
        
        # æ‰¾å‡ºä¸‹æ¶çš„ç‰©ä»¶
        removed_properties = []
        for key, previous_prop in previous_map.items():
            if key not in current_map:
                removed_properties.append(previous_prop)
        
        # è¨ˆç®—è®ŠåŒ–
        change = len(current_properties) - len(previous_data)
        
        return {
            'has_previous_data': True,
            'new_properties': new_properties,
            'removed_properties': removed_properties,
            'price_changed_properties': price_changed_properties,
            'unchanged_properties': unchanged_properties,
            'total_new': len(new_properties),
            'total_removed': len(removed_properties),
            'total_price_changed': len(price_changed_properties),
            'current_count': len(current_properties),
            'previous_count': len(previous_data),
            'change': change,
            'message': f'èˆ‡æ˜¨å¤©æ¯”è¼ƒï¼šæ–°å¢ {len(new_properties)} å€‹ã€ä¸‹æ¶ {len(removed_properties)} å€‹ã€è®Šåƒ¹ {len(price_changed_properties)} å€‹ç‰©ä»¶'
        }
    
    def upload_to_notion(self, properties: List[Dict[str, Any]], comparison: Dict = None) -> bool:
        """ä¸Šå‚³åˆ° Notion"""
        notion_token = os.getenv('NOTION_API_TOKEN')
        
        if not notion_token:
            print("âš ï¸  æœªè¨­å®š NOTION_API_TOKENï¼Œè·³é Notion ä¸Šå‚³")
            return False
        
        if not Property:
            print("âš ï¸  ç„¡æ³•è¼‰å…¥ Property æ¨¡å‹ï¼Œè·³é Notion ä¸Šå‚³")
            return False
        
        try:
            # åªä¸Šå‚³æœ‰è®ŠåŒ–çš„ç‰©ä»¶ï¼ˆå¦‚æœæœ‰å‰ä¸€å¤©è³‡æ–™ï¼‰
            properties_to_upload = []
            
            if comparison and comparison.get('has_previous_data'):
                # æ–°å¢ç‰©ä»¶
                if comparison.get('new_properties'):
                    properties_to_upload.extend(comparison['new_properties'])
                
                # åƒ¹æ ¼è®Šå‹•ç‰©ä»¶
                if comparison.get('price_changed_properties'):
                    for change_info in comparison['price_changed_properties']:
                        prop = change_info['property'].copy()
                        change_amount = change_info['change']
                        change_emoji = "ğŸ“ˆ" if change_amount > 0 else "ğŸ“‰"
                        prop['title'] = f"{prop['title']} {change_emoji} åƒ¹æ ¼è®Šå‹•: {change_info['old_price']:,}â†’{change_info['new_price']:,}è¬"
                        properties_to_upload.append(prop)
                
                if not properties_to_upload:
                    print("âœ… æ²’æœ‰æ–°å¢æˆ–è®Šå‹•çš„ç‰©ä»¶ï¼ŒNotion ç­†è¨˜ä¿æŒä¸è®Š")
                    return True
            else:
                # é¦–æ¬¡åŸ·è¡Œï¼Œä¸Šå‚³æ‰€æœ‰ç‰©ä»¶
                properties_to_upload = properties
            
            print(f"ğŸ“ æº–å‚™ä¸Šå‚³ {len(properties_to_upload)} å€‹ç‰©ä»¶åˆ° Notion")
            
            # è½‰æ›ç‚º Property ç‰©ä»¶
            property_objects = []
            for i, prop_dict in enumerate(properties_to_upload):
                prop_id = f"taipei_{i+1}_{prop_dict.get('title', '')[:10]}"
                
                prop = Property(
                    id=prop_id,
                    title=prop_dict.get('title', ''),
                    address=prop_dict.get('address', ''),
                    district=self.district_name,
                    region=self.region_name,
                    price=int(prop_dict.get('price', 0)),
                    room_count=prop_dict.get('room_count', 3),
                    living_room_count=prop_dict.get('living_room_count', 2),
                    bathroom_count=prop_dict.get('bathroom_count', 2),
                    size=prop_dict.get('size', 0),
                    floor=str(prop_dict.get('floor', '')),
                    source_site="ä¿¡ç¾©æˆ¿å±‹",
                    source_url=prop_dict.get('source_url', ''),
                    property_type='sale'
                )
                
                prop.total_price = int(prop_dict.get('price', 0))
                
                if 'main_area' in prop_dict:
                    prop.main_area = prop_dict['main_area']
                
                property_objects.append(prop)
            
            # å»ºç«‹ Notion å®¢æˆ¶ç«¯ä¸¦ä¸Šå‚³
            client = create_full_notion_client(notion_token)
            
            if not client.test_connection():
                print("âŒ Notion API é€£æ¥å¤±æ•—")
                return False
            
            success = client.create_district_house_list(
                properties=property_objects,
                search_date=datetime.now(),
                district_name="å°åŒ—",
                comparison=comparison
            )
            
            if success:
                print("âœ… æˆåŠŸä¸Šå‚³åˆ° Notionï¼")
                return True
            else:
                print("âŒ Notion ä¸Šå‚³å¤±æ•—")
                return False
        
        except Exception as e:
            print(f"âŒ Notion ä¸Šå‚³éŒ¯èª¤: {str(e)}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """ä¸»ç¨‹å¼"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¿¡ç¾©æˆ¿å±‹å°åŒ—å…¬å¯“ç‰©ä»¶çˆ¬èŸ²')
    parser.add_argument('district', 
                       nargs='?',
                       default='taipei',
                       help='åªæ”¯æ´å°åŒ—å€åŸŸ')
    
    args = parser.parse_args()
    
    if args.district not in ['taipei']:
        print("âŒ æ­¤çˆ¬èŸ²åªæ”¯æ´å°åŒ—å€åŸŸ")
        print("ğŸ’¡ å¦‚éœ€è˜†æ´²æˆ–ä¸‰é‡ï¼Œè«‹ä½¿ç”¨: python sanchong_luzhou_crawler.py")
        return
    
    print("ğŸ  ä¿¡ç¾©æˆ¿å±‹å°åŒ—å…¬å¯“ç‰©ä»¶çˆ¬èŸ²")
    print("=" * 50)
    
    try:
        crawler = TaipeiApartmentCrawler()
        
        # 1. è¼‰å…¥å‰ä¸€å¤©çš„è³‡æ–™
        print("ğŸ“‚ è¼‰å…¥å‰ä¸€å¤©çš„è³‡æ–™...")
        previous_data = crawler.load_previous_data()
        
        # 2. çˆ¬å–ä»Šå¤©çš„è³‡æ–™
        properties = crawler.crawl_all_pages()
        
        if not properties:
            print("âŒ æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•è³‡æ–™")
            return
        
        print(f"ğŸ“ˆ é¦–æ¬¡çˆ¬å–ï¼Œæ‰€æœ‰ç‰©ä»¶éƒ½æ˜¯æ–°çš„")
        
        # 3. èˆ‡å‰ä¸€å¤©æ¯”è¼ƒ
        comparison = crawler.compare_with_previous(properties, previous_data)
        print(f"ğŸ“ˆ {comparison['message']}")
        
        # é¡¯ç¤ºè©³ç´°æ¯”è¼ƒè³‡è¨Š
        print(f"\nğŸ” å°åŒ—å€åŸŸè©³ç´°æ¯”è¼ƒè³‡è¨Š:")
        print(f"  â€¢ has_previous_data: {comparison.get('has_previous_data', False)}")
        print(f"  â€¢ å‰ä¸€å¤©ç‰©ä»¶æ•¸: {comparison.get('previous_count', 0)}")
        print(f"  â€¢ ä»Šå¤©ç‰©ä»¶æ•¸: {comparison.get('current_count', 0)}")
        print(f"  â€¢ æ–°å¢ç‰©ä»¶æ•¸: {comparison.get('total_new', 0)}")
        print(f"  â€¢ ä¸‹æ¶ç‰©ä»¶æ•¸: {comparison.get('total_removed', 0)}")
        print(f"  â€¢ è®Šåƒ¹ç‰©ä»¶æ•¸: {comparison.get('total_price_changed', 0)}")
        
        if comparison.get('new_properties'):
            print(f"  ğŸ†• æ–°å¢ç‰©ä»¶ç¯„ä¾‹:")
            for i, prop in enumerate(comparison['new_properties'][:3], 1):
                print(f"    {i}. {prop['title'][:30]}... - {prop['price']}è¬")
        
        if comparison.get('price_changed_properties'):
            print(f"  ğŸ’° è®Šåƒ¹ç‰©ä»¶ç¯„ä¾‹:")
            for i, change in enumerate(comparison['price_changed_properties'][:2], 1):
                prop = change['property']
                print(f"    {i}. {prop['title'][:30]}... : {change['old_price']} â†’ {change['new_price']}è¬")
        
        if not comparison.get('new_properties') and not comparison.get('price_changed_properties'):
            print(f"  âœ… å°åŒ—å€åŸŸä»Šå¤©æ²’æœ‰æ–°å¢æˆ–è®Šåƒ¹ç‰©ä»¶")
        
        # 4. å„²å­˜æœ¬åœ°æª”æ¡ˆ
        json_file = crawler.save_to_local_file(properties)
        print(f"ğŸ“ å·²å„²å­˜åˆ°: {json_file}")
        
        # 5. ä¸Šå‚³åˆ° Notion
        print("ğŸ”„ æ­£åœ¨ä¸Šå‚³åˆ° Notion...")
        notion_success = crawler.upload_to_notion(properties, comparison)
        
        if notion_success:
            print("âœ… æˆåŠŸä¸Šå‚³åˆ° Notionï¼")
        else:
            print("âš ï¸  Notion ä¸Šå‚³å¤±æ•—æˆ–è·³é")
        
        print(f"ğŸ‰ å°åŒ—å€åŸŸçˆ¬å–å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ å°åŒ—å€åŸŸåŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
